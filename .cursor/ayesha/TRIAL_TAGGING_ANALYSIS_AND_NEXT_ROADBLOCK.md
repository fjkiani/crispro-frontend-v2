# TRIAL TAGGING ANALYSIS & NEXT ROADBLOCK RESOLUTION

**Date:** January 8, 2025  
**Status:** üîç **ANALYSIS COMPLETE** ‚Üí **NEXT ROADBLOCK IDENTIFIED**  
**Focus:** Understand trial tagging/seeding/validation process, then resolve next roadblock

---

## üö® Reality Check (Post-Implementation Update)

### What actually happened when we ‚Äútagged 500 trials‚Äù

- The batch tagging run **did execute** and tagged **~500 trials** into `api/resources/trial_moa_vectors.json`.
- **Selection was NOT patient-aligned.** The script selected **‚Äúuntagged + recruiting-ish + has interventions‚Äù** from SQLite and tagged whatever came next.
- **Implication:** this can burn LLM quota on trials that will never be shown to Ayesha (or any patient) and creates false confidence that ‚Äúmore tags = better patient experience.‚Äù

### Why we missed it

We conflated two different jobs:
1. **Offline enrichment (tagging MoA vectors)**: a background pipeline step (cheap to serve later).
2. **Patient matching (Ayesha‚Äôs trials)**: a real-time / cached per-patient experience driven by disease, location, line-of-therapy, and biomarkers.

Tagging is **not** patient matching. Tagging only becomes useful when **patient-specific candidate discovery** + **freshness guarantees** + **ranking** exist.

---

## üî¨ **TRIAL TAGGING/SEEDING/VALIDATION PROCESS - ANALYSIS**

### **Current State (59 Validated Trials)**

**Location:** `oncology-coPilot/oncology-backend-minimal/api/resources/trial_moa_vectors.json`

**Sources:**
- **`manual_intelligence_report`**: Trials tagged from Ayesha intelligence reports (high confidence, reviewed by Zo)
- **`manual_keyword_matching`**: Trials tagged using keyword matching (medium confidence)

**Total:** 59 trials (not 47 as mentioned in audit - updated count)

**Breakdown:**
- **31 DDR trials** (DDR > 0.5) - Will rank highest for Ayesha
- **6 MAPK trials**
- **3 VEGF trials**
- **3 HER2 trials**
- **6 IO trials**
- **10 Mixed/Other**

---

## üìã **HOW TRIAL TAGGING WORKS**

### **Automatic LLM-Based Validation Mechanism** ‚≠ê **KEY FEATURE**

**How It Works:**
1. **LLM Self-Reports Confidence**: The OpenAI GPT-4o model analyzes trial data and returns both:
   - MoA vector (7D: DDR, MAPK, PI3K, VEGF, HER2, IO, Efflux)
   - **Confidence score** (0.0-1.0) indicating how certain it is about the assignment

2. **Prompt Engineering for Self-Validation**:
   ```
   Rules:
   1. Only assign values > 0.0 if there is clear evidence of that mechanism
   2. If multiple mechanisms, assign proportional values (e.g., 0.6 DDR + 0.4 IO)
   3. If uncertain, use lower confidence (< 0.7)  ‚Üê KEY: LLM self-validates uncertainty
   4. Return ONLY valid JSON, no markdown formatting
   5. If no clear mechanism, return all zeros with confidence 0.0  ‚Üê KEY: LLM flags unclear cases
   ```

3. **Automatic Filtering by Confidence (useful but not sufficient)**:
   - **High confidence (>0.7)**: Reliable tags, use directly in mechanism-fit ranking
   - **Medium confidence (0.5-0.7)**: Use with caution, can be included but flagged
   - **Low confidence (<0.5)**: Uncertain tags, automatically excluded or flagged for review

4. **Production Rule (do not assume ‚Äúno review required‚Äù)**:
   - LLM confidence is a **helpful signal**, but production needs **spot checks + drift monitoring**.
   - Minimum requirement: **sample-based QA** (e.g., 30 random diverse trials per batch) + error-rate tracking.
   - Confidence thresholds must be enforced in serving layers (matching/ranking) so low-confidence tags don‚Äôt silently degrade patient results.

**Example:**
```json
{
  "NCT04284969": {
    "moa_vector": {"ddr": 0.95, ...},
    "confidence": 0.95,  ‚Üê LLM self-reports high confidence
    "source": "openai_batch_tagging",
    "provenance": {
      "model": "gpt-4o",
      "primary_moa": "DNA Damage Repair (PARP + ATR inhibitors)"
    }
  }
}
```

**Why This Works:**
- LLM is trained to recognize uncertainty and report it via confidence scores
- Prompt explicitly instructs LLM to use lower confidence when uncertain
- System automatically filters unreliable tags based on confidence thresholds
- High-confidence tags (>0.7) have been validated by the LLM itself

---

### **Method 1: Manual Intelligence Reports** (Current - High Quality)

**Process:**
1. Intelligence reports generated for Ayesha (`.cursor/ayesha/zo_intelligence_reports/`)
2. Manual review extracts MoA from trial descriptions
3. MoA vector assigned (7D: DDR, MAPK, PI3K, VEGF, HER2, IO, Efflux)
4. Stored in `trial_moa_vectors.json` with:
   - `source: "manual_intelligence_report"`
   - `confidence: 0.95` (high)
   - `reviewed_by: "Zo"`
   - `provenance.intelligence_report: "path/to/report.md"`

**Example:**
```json
{
  "NCT04284969": {
    "moa_vector": {"ddr": 0.95, "mapk": 0.0, ...},
    "confidence": 0.95,
    "source": "manual_intelligence_report",
    "reviewed_by": "Zo",
    "provenance": {
      "intelligence_report": ".cursor/ayesha/zo_intelligence_reports/INTELLIGENCE_NCT04284969_TOP_TIER.md",
      "primary_moa": "DNA Damage Repair (PARP + ATR inhibitors)"
    }
  }
}
```

**Validation:** ‚úÖ **HIGH** - Manual review by Zo, intelligence reports provide evidence

---

### **Method 2: Manual Keyword Matching** (Current - Medium Quality)

**Process:**
1. Extract interventions from trial data
2. Match keywords to MoA pathways (e.g., "PARP" ‚Üí DDR, "bevacizumab" ‚Üí VEGF)
3. Assign MoA vector based on keyword matches
4. Store with:
   - `source: "manual_keyword_matching"`
   - `confidence: 0.75-0.85` (medium)
   - `provenance.keywords_matched: ["ddr:atr", "vegf:bevacizumab"]`

**Example:**
```json
{
  "NCT02244879": {
    "moa_vector": {"ddr": 0.9, "mapk": 0.0, ...},
    "confidence": 0.85,
    "source": "manual_keyword_matching",
    "provenance": {
      "keywords_matched": ["ddr:atr"],
      "primary_moa": "DDR pathway (atr)"
    }
  }
}
```

**Validation:** ‚ö†Ô∏è **MEDIUM** - Keyword matching is deterministic but may miss nuanced mechanisms

---

### **Method 3: Batch Tagging Script** (Ready - Not Yet Executed)

**Location:** `oncology-coPilot/oncology-backend-minimal/scripts/trials/tag_trials_moa_batch.py`

**Process (as implemented):**
1. **Extract untagged trials** from SQLite database (`clinical_trials.db`)
2. **Prioritize recruiting/active trials** (status priority)
3. **Tag each trial** via LLM prompt (provider-agnostic; Cohere/Gemini/OpenAI)
4. **Parse MoA vector** from response
5. **Store with metadata** (model, provider, parsed_at, reviewed_by, source_checksum)

**Critical Limitation (must be fixed for production):**
- This process **does not** filter to Ayesha or any patient.
- It is an *enrichment job* only.
- If we want ‚ÄúAyesha logs in tomorrow ‚Üí sees all relevant trials,‚Äù tagging must be driven by patient candidate sets or by a curated disease corpus (not ‚Äúwhatever is untagged‚Äù).

**Gemini Prompt Template:**
```
Given the following clinical trial information, determine the mechanism of action (MoA) vector.

Trial: {title}
Interventions: {interventions}
Conditions: {conditions}

Return a JSON object with MoA vector (7D):
{
  "ddr": 0.0-1.0,      // DNA Damage Repair
  "mapk": 0.0-1.0,     // RAS/MAPK pathway
  "pi3k": 0.0-1.0,     // PI3K/AKT pathway
  "vegf": 0.0-1.0,     // Angiogenesis
  "her2": 0.0-1.0,     // HER2 pathway
  "io": 0.0-1.0,       // Immunotherapy
  "efflux": 0.0-1.0    // Drug efflux
}

Confidence: 0.0-1.0
Primary MoA: "description"
```

**Status (post-implementation):**
- ‚úÖ Script runs and can tag 500+ trials.
- ‚ö†Ô∏è Efficiency risk: ‚Äú1 request per trial‚Äù can exhaust monthly quotas quickly.
  - Production requires **batch prompting** (multiple trials per LLM call) + **incremental tagging** (only changed/new trials).

**Automatic LLM Validation:**
- ‚úÖ **LLM self-reports confidence** (0.0-1.0) for each MoA assignment
- ‚úÖ **Prompt engineering** ensures LLM uses lower confidence when uncertain (Rule 3: "If uncertain, use lower confidence (< 0.7)")
- ‚úÖ **Automatic filtering** based on confidence thresholds:
  - High confidence (>0.7): Use directly
  - Medium confidence (0.5-0.7): Flag for optional review
  - Low confidence (<0.5): Exclude or flag
- ‚úÖ **No human review required** - LLM confidence provides automatic validation

---

## üéØ **TRIAL SEEDING PROCESS (PLUMBER OWNERSHIP)**

### **How Trials Are Seeded**

**Location:** `oncology-coPilot/oncology-backend-minimal/data/clinical_trials.db`

**Seeding Scripts:**
1. `bulk_seed_trials.py` - Bulk seeding from ClinicalTrials.gov
2. `seed_trials_table.py` - Individual trial seeding
3. `seed_trials_standalone.py` - Standalone seeding utility
4. `seed_trials_simple.py` - Simple seeding script

**Process:**
1. **Query ClinicalTrials.gov API** for trials
2. **Extract trial data** (title, status, phase, interventions, conditions, eligibility)
3. **Store in SQLite** (`trials` table)
4. **Current database:** 1,397+ trials

**Quality Validation (production requirements):**
- Must store **seed provenance**: query used, date seeded, ctgov version fields, and a ‚Äúlast_refreshed_at‚Äù.
- Must store **patient-facing freshness**: status + locations refreshed within SLA.
- Must define corpus strategy:
  - **Ayesha-first**: keep a curated ovarian/gynecologic corpus for NYC metro with high refresh cadence.
  - **Universal**: maintain top cancer corpora (requires truly universal disease filters; current ‚Äúuniversal‚Äù pipeline is ovarian-biased).

---

## ‚úÖ **VALIDATION PROCESS**

### **Current Validation (59 Trials)**

**What's Validated:**
- ‚úÖ **Mechanism fit discrimination:** 0.983 for DDR-high patients vs 0.038 for non-DDR
- ‚úÖ **Separation:** 0.937 Œî between DDR and non-DDR trials (exceeds 0.60 target by 56.2%)
- ‚úÖ **Ranking accuracy:** Top-3 = 1.00, MRR = 0.75 (both exceed targets)
- ‚úÖ **31 DDR trials** - Sufficient for validation

**Validation Scripts:**
1. `validate_mechanism_trial_matching.py` - Core functionality (8/8 tasks passed)
2. `validate_092_mechanism_fit_claim.py` - Mechanism fit claim verification
3. `validate_mbd4_tp53_mechanism_capabilities.py` - End-to-end integration

**Evidence:**
- **Publication:** `publications/02-trial-matching/` - Validated mechanism fit ranking
- **Receipts:** `publications/02-trial-matching/receipts/latest/` - Receipt-backed metrics
- **Validation Report:** `.cursor/MOAT/CLINICAL_TRIALS/VALIDATION_REPORT.md`

---

## üìä **CURRENT VALIDATED TRIALS - FULL REASONING & EVIDENCE**

### **Top 10 DDR Trials for Ayesha (Mechanism-Fit Ranked)**

| Rank | NCT ID | Title | DDR | Mechanism Fit | Evidence | Validation |
|------|--------|-------|-----|---------------|----------|------------|
| 1 | **NCT04284969** | PARP + ATR Inhibitor Trial | 0.95 | 0.95 | Intelligence report, manual review | ‚úÖ **VALIDATED** |
| 2 | **NCT02655016** | PARP + Ceralasertib ATR | 0.95 | 0.95 | Intelligence report, manual review | ‚úÖ **VALIDATED** |
| 3 | **NCT04001023** | PARP Olaparib | 0.90 | 0.90 | Intelligence report, manual review | ‚úÖ **VALIDATED** |
| 4 | **NCT02244879** | ATR Inhibitor | 0.90 | 0.90 | Keyword: "ddr:atr" | ‚ö†Ô∏è **KEYWORD** |
| 5 | **NCT03735979** | ATR Inhibitor | 0.90 | 0.90 | Keyword: "ddr:atr" | ‚ö†Ô∏è **KEYWORD** |
| 6 | **NCT00072579** | ATM Inhibitor | 0.90 | 0.90 | Keyword: "ddr:atm" | ‚ö†Ô∏è **KEYWORD** |
| 7 | **NCT05042479** | DDR Pathway | 0.90 | 0.90 | Keyword matching | ‚ö†Ô∏è **KEYWORD** |
| 8 | **NCT02680379** | DDR Pathway | 0.90 | 0.90 | Keyword matching | ‚ö†Ô∏è **KEYWORD** |
| 9 | **NCT00652379** | DDR Pathway | 0.90 | 0.90 | Keyword matching | ‚ö†Ô∏è **KEYWORD** |
| 10 | **NCT04850079** | DDR Pathway | 0.90 | 0.90 | Keyword matching | ‚ö†Ô∏è **KEYWORD** |

### **Evidence Breakdown:**

#### **1. NCT04284969 (DDR=0.95) - HIGHEST VALIDATION**
- **Source:** `manual_intelligence_report`
- **Intelligence Report:** `.cursor/ayesha/zo_intelligence_reports/INTELLIGENCE_NCT04284969_TOP_TIER.md`
- **Primary MoA:** "DNA Damage Repair (PARP + ATR inhibitors)"
- **Confidence:** 0.95
- **Reviewed By:** Zo
- **Why for Ayesha:** MBD4+TP53 = DDR deficiency ‚Üí PARP+ATR targets both pathways
- **Validation:** ‚úÖ **MANUAL REVIEW** - Highest confidence

#### **2. NCT02655016 (DDR=0.95) - HIGHEST VALIDATION**
- **Source:** `manual_intelligence_report`
- **Intelligence Report:** `.cursor/ayesha/zo_intelligence_reports/INTELLIGENCE_NCT02655016_TOP_TIER.md`
- **Primary MoA:** "DNA Damage Repair (PARP + Ceralasertib ATR inhibitor)"
- **Confidence:** 0.95
- **Reviewed By:** Zo
- **Why for Ayesha:** PARP blocks DNA repair, ATR blocks checkpoint escape ‚Üí dual synthetic lethality
- **Validation:** ‚úÖ **MANUAL REVIEW** - Highest confidence

#### **3. NCT04001023 (DDR=0.90) - HIGH VALIDATION**
- **Source:** `manual_intelligence_report`
- **Intelligence Report:** `.cursor/ayesha/zo_intelligence_reports/INTELLIGENCE_NCT04001023_TOP_TIER.md`
- **Primary MoA:** "DNA Damage Repair (PARP inhibitor Olaparib)"
- **Confidence:** 0.90
- **Reviewed By:** Zo
- **Why for Ayesha:** PARP monotherapy for HRD+ patients (MBD4 detected = HRD+)
- **Validation:** ‚úÖ **MANUAL REVIEW** - High confidence

#### **4-10. Keyword-Matched Trials (DDR=0.90) - MEDIUM VALIDATION**
- **Source:** `manual_keyword_matching`
- **Confidence:** 0.85
- **Method:** Keyword matching (e.g., "ddr:atr", "ddr:atm")
- **Why for Ayesha:** DDR-targeting mechanisms align with patient's DDR-high profile
- **Validation:** ‚ö†Ô∏è **KEYWORD MATCHING** - Medium confidence, deterministic but may miss nuances

---

## üöß **NEXT ROADBLOCK: SHOULD WE SEED MORE TRIALS?**

### **Analysis:**

**Current State:**
- ‚úÖ **59 trials tagged** (59 validated, not 47)
- ‚úÖ **31 DDR trials** (sufficient for Ayesha)
- ‚úÖ **Mechanism fit ranking validated** (0.983 for DDR-high patients)
- ‚ö†Ô∏è **Coverage:** 59/1,397 = 4.2% (low coverage)

**Gap:**
- **Manager P3 Requirement:** 200+ trials with ‚â•90% accuracy
- **Current:** 59 trials (29.5% of target)
- **Missing:** 141+ trials to reach 200

**Options:**

### **Option 1: Seed/Tag More Trials Blindly (NOT RECOMMENDED for production)**

**Why this is wrong (what we learned):**
- You can tag 500 trials and still not guarantee Ayesha sees the right trials tomorrow.
- You can burn quota and still have stale trials if refresh isn‚Äôt enforced.
- This is enrichment without a patient experience contract.

**Automatic LLM Validation Approach:**
- ‚úÖ **LLM self-reports confidence** (0.0-1.0) in MoA assignment as part of its response
- ‚úÖ **Prompt engineering** ensures LLM uses lower confidence when uncertain:
  - Rule 3: "If uncertain, use lower confidence (< 0.7)"
  - Rule 5: "If no clear mechanism, return all zeros with confidence 0.0"
- ‚úÖ **Automatic filtering** based on confidence thresholds:
  - High confidence (>0.7): Reliable tags, use directly
  - Medium confidence (0.5-0.7): Use with caution, flag for optional review
  - Low confidence (<0.5): Uncertain tags, exclude or flag
- ‚úÖ **No human review required** - LLM confidence scores provide automatic validation

**How LLM Validation Works:**
1. **LLM analyzes** trial interventions, conditions, and description
2. **LLM assigns MoA vector** and **self-reports confidence** (0.0-1.0) in its response
3. **System automatically filters/ranks** tags by confidence score
4. **Low confidence tags** are automatically flagged or excluded
5. **High confidence tags** (>0.7) are used directly without human review

**Action:**
```bash
cd oncology-coPilot/oncology-backend-minimal
export OPENAI_API_KEY="your-key"
python3 scripts/trials/tag_trials_moa_batch.py --limit 200 --batch-size 50
```

**Timeline:** 10-15 minutes (fully automatic, no human review needed)

---

### **Option 2: Use Current 59 Trials (Sufficient for Now)**

**Pros:**
- ‚úÖ All 59 trials validated (manual review or keyword matching)
- ‚úÖ 31 DDR trials sufficient for Ayesha (DDR-high patient)
- ‚úÖ Mechanism fit ranking validated (0.983 for DDR-high)
- ‚úÖ No additional cost/time

**Cons:**
- ‚ö†Ô∏è Only 4.2% coverage (59/1,397)
- ‚ö†Ô∏è May miss better trials for other patients
- ‚ö†Ô∏è Doesn't meet Manager P3 requirement (200+ trials)

**Action:** Continue with current 59 trials, expand later

**Timeline:** Immediate (no action needed)

---

### **Option 3: Production Approach (RECOMMENDED)**

**Production contract: ‚ÄúAyesha logs in tomorrow ‚Üí sees all relevant trials, never stale.‚Äù**

This requires 4 separable concerns:

1) **Refresh (Never stale)**
- Refresh trial status/locations on schedule + on-demand for ‚Äútrials we show Ayesha‚Äù.
- Persist refresh timestamps and expose staleness in API responses.

2) **Candidate discovery (What trials are even in scope)**
- Use the agent framework (`api/services/trials/` + autonomous search) and/or a curated local corpus.
- Candidate discovery should be disease/geo/line-aware (Ayesha = ovarian/HGSOC, NYC metro).

3) **Offline tagging (MoA enrichment, provider-agnostic)**
- Tag only the candidates or curated corpus.
- Tag incrementally using a checksum (only changed/new trials).
- Use batch prompting (multiple trials per request) to reduce LLM calls.

4) **Patient matching + dossier**
- Use `trial_intelligence` / `trial_intelligence_universal` pipeline stages (hard filters ‚Üí trial type ‚Üí location ‚Üí eligibility ‚Üí LLM analysis ‚Üí dossier).
- Rank using MoA vectors (offline store) + eligibility.
- Always include provenance: why matched, freshness, and tag confidence.

---

## üß∞ Plumber Execution Plan (Dirty Work Checklist)

### A) Make trial refresh production-grade (never stale)
- Implement scheduled refresh job using `api/services/trial_refresh/`:
  - Input: ‚Äútrials shown to Ayesha in last N days‚Äù + ‚Äúpinned trials‚Äù
  - Output: update SQLite fields for status, locations, last_refreshed_at
- Add SLA policy:
  - If last_refreshed_at > 24h: mark `stale=true` and enqueue refresh
  - On Ayesha login: refresh top K displayed trials (bounded)
- Add monitoring:
  - refresh success rate, ctgov error rate, average age of displayed trials

### B) Fix corpus strategy so we don‚Äôt burn quota
- Create curated candidate sets:
  - `ovarian_hgsoc_nyc` corpus (Ayesha-first)
  - Future: ‚Äútop 5 cancers‚Äù corpora (requires truly universal disease filters)
- Store provenance for seeds:
  - query string, date seeded, ctgov fields, result count

### C) Production tagging pipeline (efficient + incremental)
- Replace ‚Äú1 LLM call per trial‚Äù with **batch prompting**:
  - 10‚Äì25 trials per request (bounded tokens)
  - Parse array output keyed by NCT ID
- Add incremental tagging via checksum:
  - Compute checksum from title + interventions + summary + conditions
  - Only re-tag if checksum changed
- Enforce quality gates:
  - store confidence; do not serve tags below threshold without flagging
  - add batch QA sampling output for review

### D) Make matching consume offline tags (no provider assumptions)
- Ensure all matching routes consume `api/resources/trial_moa_vectors.json` (offline store) first.
- Runtime fallback allowed only when explicitly enabled.

### E) Expose Ayesha trial experience as a stable API contract
- Endpoint should return:
  - trials + eligibility + mechanism_fit + confidence + freshness timestamps
  - ‚Äúwhy matched‚Äù explanation
  - staleness indicators + last refreshed

---

## üß© Production-Ready + Universal Plan (Agent Runbook)

### Goal (production tomorrow)

**Any user logs in tomorrow ‚Üí sees:**
- **All trials in-scope** for their profile (not ‚Äúwhatever we tagged‚Äù)
- **Never-stale status + locations** (explicit freshness + staleness flags)
- **Aligned ranking** (eligibility + mechanism fit)
- **Transparent provenance** (why included, why ranked, what data was used, how fresh it is)

This is achieved by 4 separable concerns (same as above), but now with the **agent task breakdown + exact wiring**.

---

## 1) Concern A ‚Äî Candidate Discovery (what‚Äôs in scope)

### Purpose
Turn a patient profile into a **bounded candidate set** of NCTs (e.g., 200‚Äì1000 trials) *before* we do any heavy work (tagging, eligibility parsing, dossier generation).

### Inputs (minimum viable)
- disease/cancer type
- location constraint (NYC metro for Ayesha)
- treatment line (frontline vs recurrent)
- biomarker hints if present (HRD / BRCA / TP53 / MBD4, etc.)

### Outputs
- `candidate_trial_ids: list[str]` (NCT IDs)
- `candidate_source_provenance` (query terms, filters, timestamp)

### Agent tasks (Discovery Agent)
- **D1 ‚Äî Build profile ‚Üí search queries**
  - Use the agent framework (`api/services/trials/` routes/agent) to generate 1‚Äì3 queries from profile.
  - Include explicit query provenance: `queries[]`, `generated_from_profile`, `time`.
- **D2 ‚Äî Fetch candidates from our local store**
  - Prefer SQLite as the first pass (fast, cheap) for ‚Äúseeded corpus‚Äù.
  - If SQLite corpus insufficient, fall back to CT.gov search (bounded).
- **D3 ‚Äî Enforce scope boundaries**
  - Must produce **bounded list** and always return counts:
    - `candidates_screened`
    - `candidates_returned`
    - reason for truncation (e.g., ‚Äútop 1000 by recruiting + proximity‚Äù)

### Acceptance criteria
- For Ayesha profile: candidates must be ovarian/gynecologic + NYC metro + recruiting prioritized.
- Must return **at least K candidates** when possible (e.g., 200) or explicitly explain why not (database too small, CT.gov rate limit, etc.).

---

## 2) Concern B ‚Äî Refresh (never stale)

### Purpose
Ensure **status + locations** are fresh for any trial we might show.

### Data contract (what must exist for every returned trial)
- `status` (Recruiting / Active / Completed / Unknown)
- `locations` (sites list with city/state)
- `last_refreshed_at` timestamp
- `stale: bool` computed from SLA

### Agent tasks (Refresh Agent ‚Äî plumber-owned)
- **R1 ‚Äî Incremental refresh queue**
  - Input: NCT IDs (from candidate discovery + recently viewed + pinned).
  - Output: refreshed trial fields in SQLite.
- **R2 ‚Äî SLA policy**
  - Example SLA: ‚Äúdisplayed trials must be refreshed within 24 hours.‚Äù
  - If stale: mark stale + enqueue refresh; UI still displays but warns ‚Äústale‚Äù.
- **R3 ‚Äî Bounded refresh on login**
  - On ‚ÄúAyesha logs in‚Äù: refresh the top K trials that will be displayed (e.g., top 20) before response if feasible (timeout-bounded).
- **R4 ‚Äî Observability**
  - Metrics: refresh success rate, CT.gov error rate, median refresh age for displayed trials.

### Acceptance criteria
- Returned response always includes `last_refreshed_at` and `stale`.
- ‚ÄúNever stale‚Äù is enforced for displayed trials, not for the entire universe (bounded work).

---

## 3) Concern C ‚Äî Offline Tagging (MoA vectors) ‚Äî universal + efficient

### Purpose
Attach mechanism vectors (7D) to trials so we can do mechanism-fit ranking **without runtime LLM calls**.

### Non-negotiables
- Tagging must be **incremental** (only changed/new trials).
- Tagging must be **batch-efficient** (many trials per LLM request).
- Tagging must be **provider-agnostic** (Cohere/Gemini/OpenAI via our abstraction).

### Tagging record schema (per NCT)
Store the following in `api/resources/trial_moa_vectors.json`:
- `moa_vector` (7D: ddr/mapk/pi3k/vegf/her2/io/efflux)
- `confidence` (0‚Äì1)
- `primary_moa` string
- `provider`, `model`
- `parsed_at`
- `source_checksum` (checksum(title + interventions + conditions + summary))
- `source` (manual_intelligence_report | keyword_matching | llm_batch_tagging)
- optional `notes`

### Agent tasks (Tagging Agent ‚Äî plumber-owned)
- **T1 ‚Äî Build checksum + incremental selection**
  - Select NCTs where:
    - not tagged yet, OR
    - checksum changed since last tag, OR
    - tag confidence below threshold and the trial is in Ayesha corpus (re-tag priority)
- **T2 ‚Äî Batch prompting**
  - Batch 10‚Äì25 trials per request.
  - Response must be **machine-parseable JSON**:
    - array of `{nct_id, moa_vector, confidence, primary_moa}`
- **T3 ‚Äî Rate limits**
  - Global rate limiter at provider layer.
  - Backoff + retry on 429/5xx.
  - Hard stop when monthly quota approached (don‚Äôt ‚Äúburn to 0‚Äù).
- **T4 ‚Äî Automated QA (not manual-by-default)**
  - Sample N=30 per batch (diverse across conditions/interventions/phase).
  - Run deterministic checks:
    - confidence present
    - vector values in [0,1]
    - at least one non-zero dimension when primary_moa claims mechanism
  - Record QA stats in logs (batch error rate).

### Acceptance criteria
- Tagging 500 trials must not require 500 requests.
- Tagging must not run unless tied to a corpus (Ayesha-first) or a defined cancer corpus (universal expansion).

---

## 4) Concern D ‚Äî Patient Matching + Dossier (what the user sees)

### Purpose
Take candidates + refresh + tags and compute:
- eligibility (hard/soft)
- mechanism fit ranking
- reasoning + dossiers

### Computation steps (must be explicit in code paths)
1. **Hard filter** (disease, recruiting, geo, phase)
2. **Eligibility checklist** (hard/soft split with unknowns flagged)
3. **Mechanism fit** using MoA vectors + patient mechanism vector
4. **Ranking** = eligibility + mechanism fit (bounded and transparent)
5. **Dossier assembly** (contacts, requirements, next steps, provenance)

### Agent tasks (Matching & Dossier Agents)
- **M1 ‚Äî Mechanism vector for patient**
  - Build a patient mechanism vector from existing framework outputs (SAE + pathway scores + tumor context when present).
  - If NGS missing, use deterministic partial vector + ‚Äúawaiting NGS‚Äù provenance.
- **M2 ‚Äî Consume offline tags**
  - Matching must read `api/resources/trial_moa_vectors.json` first.
  - Do not call LLM at runtime unless explicitly enabled and bounded.
- **M3 ‚Äî Eligibility**
  - Use the pipeline stages in `trial_intelligence_universal` for eligibility + dossier assembly.
  - Unknown criteria (ECOG, organ function): flag yellow; do not auto-exclude unless hard exclusion.
- **M4 ‚Äî Scoring transparency**
  - Return score breakdown per trial:
    - eligibility score
    - mechanism fit score
    - freshness/stale flags
    - tag confidence
    - ‚Äúwhy eligible / why good fit / requirements / red flags‚Äù

### Acceptance criteria
- Ayesha sees trials ranked with:
  - mechanism fit (DDR-heavy trials rise)
  - NYC feasibility
  - recruiting status (fresh)
  - explicit unknowns (ECOG etc.)

---

## üîå End-to-End Wiring (what calls what)

### Login / Dashboard load (Ayesha)
1. Frontend calls `POST /api/ayesha/complete_care_v2`
2. Orchestrator calls trials flow:
   - candidate discovery ‚Üí refresh top-K ‚Üí matching pipeline
3. Response includes:
   - ranked trials
   - SOC card
   - CA-125 intelligence
   - ‚Äúawaiting NGS‚Äù gates

### Background (continuous freshness + enrichment)
- Nightly (or more frequent) jobs:
  - Refresh jobs for displayed/pinned trials
  - Tagging jobs for curated corpora and changed trials
  - Optional: reseed corpora from CT.gov on schedule

---

## üß± ‚ÄúMake it Universal‚Äù Checklist (explicit tasks, not summaries)

### U1 ‚Äî Stop calling it universal if disease filter is ovarian-only
- Convert `trial_intelligence_universal` disease filter to be config-driven:
  - `FilterConfig` per cancer type (ovarian, breast, lung, colorectal, melanoma, etc.)
  - Store those configs centrally and select by `patient_profile.disease`

### U2 ‚Äî Define canonical patient profile schema for trials
- Minimal schema for trial matching:
  - disease, stage, treatment line, location radius, biomarkers list, tumor_context (optional)

### U3 ‚Äî Build corpora for top cancers (only after U1)
- For each cancer type:
  - Seed a baseline corpus (e.g., 5k‚Äì20k trials over time)
  - Tag only the relevant corpora (don‚Äôt tag all SQLite blindly)
  - Attach refresh SLA for ‚Äúrecently shown‚Äù trials

### U4 ‚Äî Universal ‚Äúnever stale‚Äù policy
- Always compute stale and last_refreshed_at for displayed trials for any patient.

---

## ‚úÖ Immediate Next Steps (agents)

### Agent: Plumber (infra + reliability)
1. Implement refresh SLA + last_refreshed_at + stale flags end-to-end.
2. Add incremental tagging via checksum + batch prompting.
3. Add scheduled jobs + bounded on-login refresh for top K displayed trials.

### Agent: Trials Discovery (patient-aligned candidates)
1. Implement profile-driven candidate discovery that outputs bounded NCT list with provenance.
2. Ensure Ayesha gets NYC + ovarian/HGSOC candidate set (not ‚Äúuntagged trials‚Äù).

### Agent: Matching/Dossier
1. Ensure matching consumes offline tags first.
2. Ensure eligibility checklist + reasoning returned in API response (no silent filters).

### Agent: Frontend (Ayesha)
1. Display freshness + stale warnings.
2. Display provenance for ranking and tag confidence.
3. Provide ‚Äúrefresh now‚Äù UI affordance for a trial (bounded).


**Timeline:** 
- Phase 1: Immediate (current state)
- Phase 2: 1 week (batch tagging + validation)
- Phase 3: Ongoing (maintenance)

---

## üéØ **RECOMMENDATION: HYBRID APPROACH**

### **For Ayesha Dashboard (Now):**

**Use Current 59 Validated Trials:**
- ‚úÖ All validated (manual review or keyword matching)
- ‚úÖ 31 DDR trials sufficient for DDR-high patient
- ‚úÖ Mechanism fit ranking validated (0.983 for DDR-high)
- ‚úÖ Full reasoning and evidence available

**Present with Full Transparency:**
- Show mechanism fit scores
- Show source (manual_intelligence_report vs manual_keyword_matching)
- Show confidence scores
- Show reasoning (why DDR alignment for Ayesha)

### **For Future Expansion (Next Week):**

**Run Batch Tagging:**
- Tag 200+ trials using `tag_trials_moa_batch.py`
- Human spot-review 30 diverse trials
- Validate ‚â•90% accuracy
- Merge with existing 59 trials

**Result:** 200+ validated trials (meets Manager P3 requirement)

---

## üöß **NEXT ROADBLOCK FROM STRATEGIC AUDIT**

### **Roadblock #2: CA-125 Intelligence Validation** (Lines 435-438)

**From Strategic Audit:**
> 2. **Validate CA-125 Intelligence**
>    - Use TCGA-OV cohort (585 patients) with CA-125 data
>    - Validate burden classification, response forecast
>    - Compute precision/recall for resistance detection

**Current Status:**
- ‚ö†Ô∏è **Literature-aligned expectations** (90% confidence)
- ‚ö†Ô∏è **NOT validated** in our system
- ‚ö†Ô∏è **Risk:** Miss resistance ‚Üí delayed treatment change ‚Üí death

**What Needs to Be Done:**

1. **Load TCGA-OV Cohort with CA-125 Data**
   - Location: `oncology-coPilot/oncology-backend-minimal/biomarker_enriched_cohorts/data/tcga_ov_enriched_v2.json`
   - 585 patients with outcomes (OS, PFS)
   - Need: CA-125 values (may not be in current dataset)

2. **Validate Burden Classification**
   - Test: Does CA-125 burden class (EXTENSIVE/MODERATE/LOW) correlate with outcomes?
   - Metric: Survival stratification (HR for EXTENSIVE vs LOW)

3. **Validate Response Forecast**
   - Test: Does forecasted CA-125 (cycle 3, cycle 6) match actual outcomes?
   - Metric: Correlation between forecasted and actual CA-125 values

4. **Validate Resistance Detection**
   - Test: Does resistance signal detection predict progression?
   - Metric: Precision/Recall for resistance detection (true positives, false positives)

**Action Plan:**
- Create validation script: `validate_ca125_intelligence.py`
- Load TCGA-OV cohort with CA-125 data (or find alternative dataset)
- Run CA-125 intelligence service on cohort
- Compare predictions vs actual outcomes
- Compute metrics (precision, recall, F1, HR)

---

## üìã **IMMEDIATE ACTIONS**

### **1. Present Current Validated Trials (Now)**

**For Ayesha Dashboard:**
- Show top 10 mechanism-fit ranked trials
- Include full reasoning:
  - Mechanism fit score
  - Source (manual_intelligence_report vs manual_keyword_matching)
  - Confidence score
  - Why DDR alignment for Ayesha (MBD4+TP53 = DDR deficiency)
  - Evidence (intelligence reports, keyword matches)

**Example Display:**
```
Trial: NCT04284969 (PARP + ATR Inhibitor)
Mechanism Fit: 0.95 (DDR alignment)
Source: Manual Intelligence Report (reviewed by Zo)
Confidence: 0.95 (high)
Why for Ayesha: MBD4+TP53 creates DDR deficiency ‚Üí PARP+ATR targets both pathways
Evidence: Intelligence report available at .cursor/ayesha/zo_intelligence_reports/INTELLIGENCE_NCT04284969_TOP_TIER.md
```

### **2. Resolve Next Roadblock: CA-125 Validation (Next)**

**Create Validation Script:**
- `oncology-coPilot/oncology-backend-minimal/scripts/validation/validate_ca125_intelligence.py`
- Load TCGA-OV cohort
- Run CA-125 intelligence service
- Compare predictions vs outcomes
- Generate validation report

**Timeline:** 2-3 hours

---

## üéØ **BOTTOM LINE**

### **Trial Tagging/Seeding/Validation:**

**Current State:**
- ‚úÖ **59 validated trials** (manual review or keyword matching)
- ‚úÖ **31 DDR trials** (sufficient for Ayesha)
- ‚úÖ **Mechanism fit ranking validated** (0.983 for DDR-high)
- ‚ö†Ô∏è **Coverage:** 4.2% (59/1,397) - low but sufficient for Ayesha

**Process:**
1. **Seeding:** Trials extracted from ClinicalTrials.gov ‚Üí SQLite database (1,397+ trials)
2. **Tagging:** Manual review (intelligence reports) or keyword matching ‚Üí `trial_moa_vectors.json`
3. **Validation:** Mechanism fit ranking validated (0.983 for DDR-high patients)

**Should We Seed More?**
- **For Ayesha:** ‚úÖ **NO** - 59 trials sufficient (31 DDR trials)
- **For Production:** ‚úÖ **YES** - Run batch tagging to reach 200+ trials (Manager P3 requirement)

### **Next Roadblock:**
- **CA-125 Intelligence Validation** - Validate burden classification, response forecast, resistance detection using TCGA-OV cohort

---

**Status:** ‚úÖ **ANALYSIS COMPLETE** ‚Üí **NEXT ROADBLOCK IDENTIFIED**  
**Next:** Create CA-125 validation script

