# ðŸŽ¯ SYNTHETIC LETHALITY MODEL IMPROVEMENT PLAN

**Problem:** S/P/E model (62.5%) underperforms rule-based baseline (75.0%)  
**Target:** Achieve >80% accuracy to beat rule-based and demonstrate ML value  
**Date:** January 28, 2025  
**Status:** Planning Phase

---

## ðŸ“Š CURRENT PERFORMANCE ANALYSIS

### Baseline Comparison Results:
- **Random Baseline:** 12.5% (1/8 correct)
- **Rule-Based Baseline:** 75.0% (6/8 correct) - **CURRENT WINNER**
- **S/P/E Model:** 62.5% (5/8 correct) - **NEEDS IMPROVEMENT**

---

## ðŸ§­ DOCTRINE UPDATE (What we need + how we get there)

### Nonâ€‘negotiables (aligned to `spe_framework_master.mdc`)
To claim â€œS/P/E beats rule baselineâ€ we must run and record the system the way the S/P/E doctrine expects:
- **Profiles must be explicit**: Baseline (**SP**) vs Full (**SPE**) vs Fusion (eligible only)
- **Flags must be explicit and logged** (reproducibility + deterministic runs):
  - `DISABLE_LITERATURE=1` (deterministic baseline; then re-enable and measure lift)
  - `EVO_FORCE_MODEL=evo2_1b` (stable, cost-controlled)
  - `EVO_USE_DELTA_ONLY=1` (stable S-signal / spam prevention)
  - `DISABLE_FUSION=1` unless we can prove GRCh38 missense coverage
- **Provenance is mandatory**: every benchmark artifact must include at least `{run_id, profile, flags}`
- **GRCh38 hygiene is mandatory**: no `ref:\"?\" alt:\"?\"` in any case where we claim coordinate-aware behavior

### How we actually beat the rule baseline
The rule baseline is strong because the dataset prior is DDRâ†’PARP heavy. We win by being **more correct** on cases the baseline cannot handle:
- DDR-but-benign negatives (baseline overcalls PARP)
- Nonâ€‘DDR positives (ATR/WEE1/CHK1 contexts)
- TP53-only checkpoint sensitivity patterns
- ARID1A pathway-specific patterns
- Within-class ranking (Drug@1) once class selection is correct

### What we need (concrete outputs)
1. **Evaluation contract** that reports: Class@1, Drug@1, Negative FP rate (not a single â€œaccuracyâ€)
2. **Hardâ€‘set suite** (10â€“15 cases) intentionally designed to make rule baseline fail
3. **Profile receipts**: SP vs SPE runs with deterministic flags and provenance logged
4. **Flagged model changes** so we can A/B test and prove lift without contaminating the baseline

---

## ðŸ”¥ KEY INSIGHTS (What the numbers actually mean)

### Insight 0: We do **not** have measurement hygiene yet
The 62.5% vs 75.0% headline is **directional**, but not publication-grade yet, because:
- The comparison was run on an **8-case pilot subset**, not the full 100.
- The baseline script is effectively evaluating **positives** (cases with `effective_drugs`) and may exclude negative controls from the denominator.
- The S/P/E â€œmodelâ€ in this comparison is currently **cache-dependent**, and the cache only covers a small subset of cases.
- We are currently mixing two different tasks:
  - **Task A (class selection):** â€œShould we recommend PARP-class at all?â€
  - **Task B (within-class ranking):** â€œWhich PARP (olaparib vs niraparib vs rucaparib)?â€

**Actionable conclusion:** Before tuning the model, we must lock an evaluation contract that:
1) uses the same denominator across all methods, 2) reports class-level and drug-level separately, and 3) includes negative controls.

### Insight 1: The rule-based baseline is â€œwinningâ€ because the dataset prior is DDR-heavy (by design)
Our pilot dataset intentionally contains many DDRâ†’PARP positives, so a heuristic that maps DDRâ†’PARP will look extremely strong.
Thatâ€™s fineâ€”**itâ€™s the correct baseline**â€”but it means S/P/E only â€œwinsâ€ if it:
- Matches DDRâ†’PARP on the easy cases **and**
- Beats the baseline on **hard cases** (TP53â†’WEE1, ARID1Aâ†’ATR, ambiguous DDR missense, and true negatives).

### Insight 2: Adding DDRâ†’PARP boost likely gets parity, not lift
An explicit DDRâ†’PARP boost closes the obvious gap, but itâ€™s effectively implementing the baseline inside the model.
Publication value comes from the next layer:
- variant-level pathogenicity gating (not all DDR hits are equal),
- pair-aware special cases (e.g., BER+checkpoint patterns),
- evidence-tier-aware confidence, and
- non-DDR SL mapping (ATR/WEE1/CHK1 contexts).

### Why Rule-Based Wins:
The rule-based baseline uses a simple heuristic:
```python
IF any DDR gene (BRCA1, BRCA2, ATM, ATR, CHEK1, CHEK2, PALB2, RAD51C, RAD51D, MBD4) 
THEN recommend "Olaparib" (PARP inhibitor)
ELSE random drug
```

**Why this works:**
- 70% of test cases (70/100) have synthetic lethality detected
- Most SL cases involve DDR genes â†’ PARP inhibitors
- Simple pattern matching beats complex ML when pattern is clear

**Why S/P/E fails:**
- Over-complicates simple DDR â†’ PARP pattern
- May miss obvious DDR gene matches
- Drug ranking may not prioritize PARP for DDR cases
- Evidence/Pathway signals may dilute clear sequence signals

---

## ðŸ” ROOT CAUSE ANALYSIS

### Issue 1: DDR Gene Detection Not Prioritized
**Problem:** S/P/E model doesn't explicitly prioritize DDR genes for PARP recommendation

**Current Flow:**
1. Sequence scoring (Evo2) â†’ variant impact
2. Pathway aggregation â†’ pathway scores
3. Evidence gathering â†’ literature/ClinVar
4. Drug scoring â†’ weighted combination

**Gap:** No explicit "IF DDR gene THEN boost PARP" rule in drug scoring

### Issue 2: S/P/E Weight Imbalance
**Current Formula:** `0.3 * S + 0.4 * P + 0.3 * E + clinvar_prior`

**Problem:**
- Pathway (40%) may not capture DDR â†’ PARP relationship strongly enough
- Sequence (30%) focuses on variant impact, not gene-level DDR membership
- Evidence (30%) may be noisy or incomplete

**Rule-based advantage:** Direct gene â†’ drug mapping (100% weight on gene match)

### Issue 3: Drug Ranking May Not Favor PARP for DDR Cases
**Problem:** Even if S/P/E detects DDR pathway disruption, drug ranking may not prioritize PARP inhibitors

**Current Drug Scoring:**
- Scores all drugs equally
- No explicit "DDR pathway â†’ PARP" boost
- May rank other drugs (e.g., platinum) higher

### Issue 4: Limited Test Case Coverage
**Problem:** Only 10 cached responses (8 test cases) - may not represent full 100-case distribution

**Impact:** Results may be skewed by specific cases

---

## ðŸŽ¯ IMPROVEMENT STRATEGY

### Phase 0: Fix measurement + make the benchmark â€œabout the real questionâ€ (same day)

#### 0.1 Report two metrics (we need both for publication)
- **Class@1**: correct if predicted drug belongs to the correct class (e.g., any PARP matches PARP class)
- **Drug@1**: correct only if predicted drug matches one of the ground-truth drugs
- **Negatives FP rate**: % negative controls that still get a PARP recommendation

#### 0.2 Force identical denominators across all methods
Random / Rule / S/P/E must be scored on the **same cases**.
If S/P/E is missing a prediction (e.g., cache miss), that must be treated as wrong **unless** we exclude that case for all methods.

#### 0.3 Create a â€œhard-setâ€ mini-suite (10â€“15 cases) where the rule baseline should fail
We need a small suite that demonstrates lift beyond DDRâ†’PARP:
- TP53-only checkpoint cases (WEE1/ATR/CHK1 compete)
- ARID1A cases (ATR sensitivity patterns)
- â€œDDR present but likely benignâ€ negatives (avoid false PARP)
- Non-DDR positives (force pathway/evidence reasoning)

**Acceptance:** On the hard-set, S/P/E beats the rule baseline by a clear margin (even if they tie on DDRâ†’PARP).

#### 0.4 Fix reporting schema drift (stop publishing wrong zeros)
We currently have a schema mismatch risk between result JSONs and reporting scripts.

**Rule:** One canonical schema for benchmark outputs, then every renderer/table generator consumes that schema.

Minimum required keys in every benchmark report:
- `summary` (metrics)
- `provenance` (`run_id`, `profile`, `flags`)
- `cases[]` (case_id, gt, prediction, pass/fail, category tags)

### Phase 1: Quick Wins (Immediate - 1-2 days)

#### 1.1 Add DDR â†’ PARP Explicit Rule
**Action:** Add explicit boost in drug scoring for DDR genes
```python
# In drug_scorer.py or efficacy orchestrator
if any_ddr_gene_present(mutations):
    for parp_drug in ["Olaparib", "Niraparib", "Rucaparib"]:
        efficacy_score += 0.2  # Boost PARP drugs
```

**Expected Impact:** +10-15% accuracy (should match rule-based baseline)

**File:** `api/services/efficacy/drug_scorer.py` or `api/routers/efficacy.py`

**Implementation constraint (so we can prove lift):**
- Put this behind a feature flag (e.g., `SL_PRIOR_BOOST=1`) and emit it in provenance/receipt.
- That lets us A/B compare: baseline S/P/E vs S/P/E+DDR-boost vs pure rule baseline.

#### 1.2 Improve DDR Gene Detection
**Action:** Expand DDR gene list and ensure all variants are detected
```python
DDR_GENES = [
    "BRCA1", "BRCA2", "ATM", "ATR", "CHEK1", "CHEK2", 
    "PALB2", "RAD51C", "RAD51D", "MBD4", "BRIP1", 
    "RAD51B", "RAD51", "FANCA", "FANCD2", "FANCI"
]
```

**Expected Impact:** +5% accuracy (catch more DDR cases)

**Key insight:** â€œDDR gene presentâ€ is not sufficientâ€”gate by variant strength:
- LoF (frameshift/stop_gained/splice) gets full DDR boost
- Missense requires ClinVar pathogenic/likely pathogenic OR high Evo2 impact percentile before we treat it as DDR-disrupting
This is the main path to beating the rule baseline on â€œDDR-but-benignâ€ negatives.

#### 1.3 Adjust S/P/E Weights for SL Context
**Action:** Increase Pathway weight when DDR pathway detected
```python
# If DDR pathway score > threshold:
pathway_weight = 0.5  # Increase from 0.4
sequence_weight = 0.3
evidence_weight = 0.2  # Decrease from 0.3
```

**Expected Impact:** +5-10% accuracy

**Key insight:** We need to verify weights are actually tunable.
If the scoring formula is hardcoded, changing config weights wonâ€™t move anything; fix that discrepancy first or directly edit the hardcoded constants.

---

## ðŸ§ª EXECUTION (How we get where we need to be without burning credits)

### Step 1: Deterministic floor (no literature, no fusion)
Run all comparisons under a deterministic profile so lift attribution is clean:
- `DISABLE_LITERATURE=1`
- `DISABLE_FUSION=1`
- `EVO_FORCE_MODEL=evo2_1b`
- `EVO_USE_DELTA_ONLY=1`

This creates a stable baseline to measure lift from:
- DDRâ†’PARP prior boost (flagged)
- DDR variant-strength gating (flagged)
- weight changes (only after confirming weights arenâ€™t hardcoded)

### Step 2: Hard-set first (prove lift where the baseline fails)
We do not need 100 cases to prove correctness.
We need a 10â€“15 case hard-set where the rule baseline fails by construction.

Acceptance:
- Rule baseline is meaningfully below S/P/E on hard-set
- S/P/E improves **Class@1** and reduces **Negative FP rate**
- **Drug@1** improves without inflating FP rate

### Step 3: Scale to 100 (then stratify)
After hard-set lift is proven, run the full 100-case suite and stratify:
- DDRâ€‘LoF
- DDRâ€‘missense pathogenic
- DDRâ€‘missense benign / VUS
- nonâ€‘DDR positives
- true negatives

### Step 4: Re-enable evidence (optional lift)
Only after deterministic correctness is locked:
- set `DISABLE_LITERATURE=0`
- measure whether evidence improves Drug@1 without increasing FP rate

---

## ðŸ§ª EXECUTION (How we get where we need to be without burning credits)

### Step 1: Deterministic floor (no literature, no fusion)
Run all comparisons under a deterministic profile so lift attribution is clean:
- `DISABLE_LITERATURE=1`
- `DISABLE_FUSION=1`
- `EVO_FORCE_MODEL=evo2_1b`
- `EVO_USE_DELTA_ONLY=1`

This creates a stable baseline to measure lift from:
- DDRâ†’PARP prior boost (flagged)
- DDR variant-strength gating (flagged)
- weight changes (only after confirming weights arenâ€™t hardcoded)

### Step 2: Hard-set first (prove lift where the baseline fails)
We do not need 100 cases to prove correctness.
We need a 10â€“15 case hard-set where the rule baseline fails by construction.

Acceptance:
- Rule baseline is meaningfully below S/P/E on hard-set
- S/P/E improves **Class@1** and reduces **Negative FP rate**
- **Drug@1** improves without inflating FP rate

### Step 3: Scale to 100 (then stratify)
After hard-set lift is proven, run the full 100-case suite and stratify:
- DDRâ€‘LoF
- DDRâ€‘missense pathogenic
- DDRâ€‘missense benign / VUS
- nonâ€‘DDR positives
- true negatives

### Step 4: Re-enable evidence (optional lift)
Only after deterministic correctness is locked:
- set `DISABLE_LITERATURE=0`
- measure whether evidence improves Drug@1 without increasing FP rate

---

## EXECUTION (How we get where we need to be without burning credits)

### Step 1: Deterministic floor (no literature, no fusion)
Run comparisons under a deterministic profile so lift attribution is clean:
- `DISABLE_LITERATURE=1`
- `DISABLE_FUSION=1`
- `EVO_FORCE_MODEL=evo2_1b`
- `EVO_USE_DELTA_ONLY=1`

This creates a stable baseline to measure lift from:
- DDRâ†’PARP prior boost (flagged)
- DDR variant-strength gating (flagged)
- weight changes (only after confirming weights arenâ€™t hardcoded)

### Step 2: Hard-set first (prove lift where the baseline fails)
We do not need 100 cases to prove correctness.
We need a 10â€“15 case hard-set where the rule baseline fails by construction.

Acceptance:
- Rule baseline is meaningfully below S/P/E on hard-set
- S/P/E improves **Class@1** and reduces **Negative FP rate**
- **Drug@1** improves without inflating FP rate

### Step 3: Scale to 100 (then stratify)
After hard-set lift is proven, run the full 100-case suite and stratify:
- DDRâ€‘LoF
- DDRâ€‘missense pathogenic
- DDRâ€‘missense benign / VUS
- nonâ€‘DDR positives
- true negatives

### Step 4: Re-enable evidence (optional lift)
Only after deterministic correctness is locked:
- set `DISABLE_LITERATURE=0`
- measure whether evidence improves Drug@1 without increasing FP rate

### Phase 2: Medium-Term Improvements (1 week)

#### 2.1 Enhance Pathway Aggregation for DDR
**Action:** Improve DDR pathway detection and scoring
- Better gene â†’ pathway mapping
- Pathway disruption thresholds
- Multi-pathway interactions (BER + HRR)

**Expected Impact:** +5-10% accuracy

#### 2.2 Improve Evidence Tier for PARP
**Action:** Boost evidence scores for FDA-approved PARP indications
- BRCA1/2 â†’ PARP: Tier 1 (FDA approved)
- Other DDR â†’ PARP: Tier 2-3 (clinical trials)
- Explicit evidence tier â†’ confidence boost

**Expected Impact:** +3-5% accuracy

#### 2.3 Better Drug Ranking for SL Context
**Action:** Context-aware drug ranking
- If DDR pathway disrupted â†’ prioritize PARP
- If HRD+ â†’ prioritize PARP
- If BER deficiency â†’ prioritize PARP

**Expected Impact:** +5-10% accuracy

**Key insight (prevents â€œbaseline = whole gameâ€):**
Split ranking into two stages:
- **Stage 1 (class ranker):** PARP vs ATR vs WEE1 vs â€¦
- **Stage 2 (within-class ranker):** olaparib vs niraparib vs rucaparib (or ATR agents)

This is how we both (a) match the strong baseline on class selection and (b) still deliver publication-grade specificity.

### Phase 3: Advanced Optimizations (2 weeks)

#### 3.1 Machine Learning Calibration
**Action:** Train calibration model on benchmark results
- Learn optimal S/P/E weights per cancer type
- Learn drug ranking preferences
- Learn pathway â†’ drug mappings

**Expected Impact:** +10-15% accuracy

#### 3.2 Multi-Modal Integration
**Action:** Combine S/P/E with synthetic lethality detection
- Use SL detection as prior
- Boost drugs that match SL pairs
- Integrate essentiality scores

**Expected Impact:** +5-10% accuracy

**New asset we now have:** Real DepMap CRISPR gene effect data is downloaded locally.
However, our current extraction is a **global mean across all cell lines**, which can be misleading for ovarian-focused claims.
Upgrade path:
- Compute **global** essentiality (all lines)
- Compute **ovarian-lineage** essentiality (filter via DepMap `Model.csv`)
- Use the lineage-specific prior in confidence / tie-breakers (not as a hard rule)

#### 3.3 Expanded Test Case Validation
**Action:** Run full 100-case benchmark with real API
- Identify failure modes
- Error analysis by gene/cancer type
- Targeted improvements

**Expected Impact:** Better understanding, +5-10% accuracy

---

## ðŸ“‹ IMPLEMENTATION CHECKLIST

### Phase 0 (Today): Measurement hygiene
- [ ] Add Class@1 vs Drug@1 scoring (report both)
- [ ] Ensure identical denominators across Random / Rule / S/P/E
- [ ] Add a â€œhard-setâ€ mini-suite (10â€“15 cases) with explicit expected outcomes
- [ ] Standardize benchmark report schema (summary + provenance + cases[])
- [ ] Standardize benchmark report schema (summary + provenance + cases[])
- [ ] Standardize benchmark report schema (summary + provenance + cases[])

### Immediate (Today):
- [ ] Add DDR â†’ PARP explicit boost in drug scoring
- [ ] Expand DDR gene list
- [ ] Test on 8-case pilot dataset
- [ ] Re-run baseline comparison

### Week 1:
- [ ] Adjust S/P/E weights for DDR context
- [ ] Improve pathway aggregation
- [ ] Enhance evidence tier logic
- [ ] Run full 100-case benchmark (cached)

### Week 2:
- [ ] ML calibration (if needed)
- [ ] Multi-modal integration
- [ ] Error analysis and targeted fixes
- [ ] Final validation

---

## ðŸŽ¯ SUCCESS METRICS

### Target Performance:
- **Minimum:** Tie or exceed rule baseline on **Class@1**
- **Target:** Beat rule baseline on the **hard-set** (where DDRâ†’PARP isnâ€™t enough)
- **Stretch:** Beat rule baseline on **Drug@1** while keeping negative-control FP rate low

### Validation:
- Run on full 100-case dataset
- Compare to baselines (random, rule-based)
- Statistical significance testing
- Error analysis by category

---

## ðŸ”§ TECHNICAL DETAILS

### Current S/P/E Formula:
```python
efficacy_score = 0.3 * seq_pct + 0.4 * path_pct + 0.3 * evidence + clinvar_prior
```

### Proposed Enhanced Formula:
```python
# Base S/P/E score
base_score = 0.3 * seq_pct + 0.4 * path_pct + 0.3 * evidence + clinvar_prior

# DDR â†’ PARP boost
if ddr_pathway_detected and drug in PARP_DRUGS:
    base_score += 0.2  # Explicit boost

# Context-aware weights
if ddr_pathway_score > 0.7:
    # Increase pathway weight for strong DDR signals
    base_score = 0.25 * seq_pct + 0.5 * path_pct + 0.25 * evidence + clinvar_prior

final_score = min(1.0, base_score)  # Cap at 1.0
```

### DDR Gene Detection:
```python
DDR_GENES = {
    "HRR": ["BRCA1", "BRCA2", "PALB2", "RAD51C", "RAD51D", "RAD51", "BRIP1", "RAD51B"],
    "BER": ["MBD4", "MUTYH", "OGG1", "NTHL1"],
    "NHEJ": ["ATM", "ATR", "CHEK1", "CHEK2", "TP53"],
    "FA": ["FANCA", "FANCD2", "FANCI", "FANCL"]
}

def detect_ddr_pathway(mutations):
    genes = [m.get("gene") for m in mutations]
    ddr_genes_found = [g for g in genes if any(g in pathway_genes for pathway_genes in DDR_GENES.values())]
    return len(ddr_genes_found) > 0, ddr_genes_found
```

---

## ðŸ“Š EXPECTED OUTCOMES

### After Phase 1 (Quick Wins):
- **Accuracy:** 75-80% (match or beat rule-based)
- **Time:** 1-2 days
- **Risk:** Low (explicit rules, easy to test)

### After Phase 2 (Medium-Term):
- **Accuracy:** 80-85%
- **Time:** 1 week
- **Risk:** Medium (requires pathway/evidence improvements)

### After Phase 3 (Advanced):
- **Accuracy:** 85-90%
- **Time:** 2 weeks
- **Risk:** Higher (ML calibration, complex integration)

---

## ðŸš€ NEXT STEPS

1. **Review this plan** - Confirm approach and priorities
2. **Implement Phase 1** - Quick wins to match rule-based
3. **Validate** - Re-run baseline comparison
4. **Iterate** - Phase 2 and 3 based on results

**Priority:** Phase 1 quick wins to demonstrate improvement path

---

**This plan is a living document - update as we learn from implementation and validation.**
