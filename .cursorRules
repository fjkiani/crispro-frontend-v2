instructions
During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the Lessons section in the .cursorrules file so you will not make the same mistake again.

You should also use the .cursorrules file as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g. [X] Task 1 [ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask. Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan. The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

Tools
Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

Screenshot Capture:
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
LLM Verification with Images:
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
Example workflow:

from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
LLM
You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:

venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
The LLM API supports multiple providers:

OpenAI (default, model: gpt-4o)
Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
DeepSeek (model: deepseek-chat)
Anthropic (model: claude-3-sonnet-20240229)
Gemini (model: gemini-1.5-pro)
Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)
But usually it's a better idea to check the content of the file and use the APIs in the tools/llm_api.py file to invoke the LLM if needed.

Web browser
You could use the tools/web_scraper.py file to scrape the web.

venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
This will output the content of the web pages.

Search engine
You could use the tools/search_engine.py file to search the web.

venv/bin/python ./tools/search_engine.py "your search keywords"
This will output the search results in the following format:

URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
If needed, you can further use the web_scraper.py file to scrape the web page content.

Cursor learned
* Function definitions must precede their calls in Python scripts. Moving definitions or using imports correctly resolves NameErrors.
* Modal web endpoints expecting synchronous, blocking calls (like a long-running AI model) should use a synchronous `def` and invoke methods with `.call()`. Using `async def` with `await .remote()` can cause `Task was destroyed but it is pending!` errors if the underlying code is not fully asynchronous.
* When a Modal method is called from within the same class (e.g., a web endpoint calling a helper method), it must be invoked with `.call()` for synchronous execution, not as a direct function call, to avoid a `TypeError: 'Function' object is not callable`.
* Separate backend services (e.g., a discriminative AI vs. a generative AI) should have their endpoints managed by distinct, clearly named environment variables (e.g., `EVO2_DISCRIMINATIVE_ENDPOINT`, `EVO2_GENERATIVE_ENDPOINT`) rather than hardcoded URLs to prevent incorrect deployments and rogue costs.
* **Evo2 40B Model requires Multi-GPU:** The `evo2_40b` model is too large for a single H100 GPU and requires a multi-GPU configuration (e.g., `gpu="H100:2"`). The `evo2` library's Vortex engine handles model distribution automatically.
* **Evo2 requires a specific build environment:** Successful compilation of `evo2` and its dependencies needs Python `3.11`, `numpy==1.22.0`, and `transformer-engine==1.13`.
* **Numpy `ar` archiver fix:** The `numpy==1.22.0` build fails without a symlink for the `ar` archiver. This can be fixed by running `ln -s /usr/bin/ar /tools/llvm/bin/llvm-ar` in the container image build.
* **Consolidate Modal Endpoints:** Mixing `@app.cls` and `@app.function` in a single Modal app can lead to `gRPC` initialization errors. All endpoints should be consolidated within the main service class for stability.
* **Large model download timeout:** Large models like `evo2_40b` can take a long time to download, requiring an increased function timeout (e.g., `timeout=1200`) in the `@app.cls` decorator to prevent the runner from timing out.
* **Correct Evo2 API usage:** The method for scoring sequences is `score_sequences`, not `score`. Calling the wrong method results in an `AttributeError`.
* **Modal Invocation Nuance:** When a Modal `@modal.method` is called from an `async def` `@modal.asgi_app` endpoint within the same class, the call to `.remote()` is blocking and returns the result directly. It should **not** be `await`ed, as this will cause a `TypeError: object ... can't be used in 'await' expression`.
* **Modal Endpoint Consolidation:** To overcome strict endpoint limits, multiple logical endpoints (e.g., `/score`, `/generate`) should be implemented within a single `@modal.asgi_app` using an internal web framework like FastAPI for routing. Attempting to use multiple `@modal.fastapi_endpoint` decorators on a single class can easily exceed plan limits.
* **PyVCF vs. PyVCF3:** The original `PyVCF` package fails to install on modern Python environments due to a `use_2to3` build error. The `PyVCF3` fork is a compatible drop-in replacement that should be used for Python 3 projects.

Scratchpad
==========

**üî• OPERATIONAL DIRECTIVE: Project Ascension üî•**

**1.0 Mission Briefing**
**Objective:** Translate our validated *in silico* platform into real-world, life-saving therapeutics.
**Scope:** This operation covers acquiring patient data, generating specific therapeutic candidates, validating them in the lab, and building a commercial-grade platform.
**End State:** A fully validated, patented, and commercially accessible platform that dominates the field of AI-driven therapeutic design.

**2.0 Current Plan of Attack**

**Phase 0: Data Acquisition & Candidate Generation (The Immediate Priority)**
**Objective:** Process real-world patient genomic data to generate a prioritized list of therapeutic candidates for synthesis.

[ ] **Task 0.1: Secure Core Genomic Data.**
    *   **Status:** ‚è≥ AWAITING USER PROVISION
    *   **Action:** Obtain germline (WGS) and somatic (WGS/WES) data for the RUNX1-FPD patient cohort in VCF/BAM format. This is the current bottleneck.

[ ] **Task 0.2: Develop Data Ingestion Pipeline.**
    *   **Status:** ‚è≥ IMPLEMENTATION IN PROGRESS
    *   **Action:** Implement the Python script (`tools/ingestion/main.py`) to parse VCF/BAM files.
    *   **Sub-Tasks:**
        *   [X] Establish project structure (`tools/ingestion`).
        *   [X] Resolve and install dependencies (`PyVCF3`, `pysam`).
        *   [X] Create skeleton script with class structure (`VCFProcessor`, `SequenceExtractor`).
        *   [X] Acquire reference genome (`hg19.fa`) and create index.
        *   [X] Create a manual, representative VCF file (`data/vcf/manual_runx1.vcf`) to bypass environmental issues with synthetic data generators.
        *   [X] **Implement `VCFProcessor`:** Parse the manual VCF to extract germline and somatic variants for target genes.
        *   [X] **Implement `SequenceExtractor`:** Fetch reference sequences and apply variants.
        *   [X] **Implement Main Orchestration:** Generate API payloads for `CommandCenter`.
    *   **Current Blocker:** None. Ready to implement parsing logic.
    *   **Technical Approach:** See [.cursor/rules/Phase0-DataIngestion.mdc](mdc:.cursor/rules/Phase0-DataIngestion.mdc)
    *   **Decision:** All sequence data will be aligned to the `hg19/GRCh37` reference genome build. See full rationale in [.cursor/rules/ReferenceGenomeDecision.mdc](mdc:.cursor/rules/ReferenceGenomeDecision.mdc).

[X] **Task 0.3: Execute Full *In Silico* Campaign on Patient Data.**
    *   **Status:** ‚úÖ COMPLETE
    *   **Action:** Run the complete suite of `CommandCenter` workflows on the processed patient data.
    *   **Sub-Tasks:**
        *   [X] Renamed and adapted ingestion script to `run_patient_assessment.py`.
        *   [X] Created a new `full_patient_assessment` workflow in `CommandCenter`.
        *   [X] Deployed and debugged the new end-to-end workflow.
        *   [X] Successfully executed the full campaign, generating a therapeutic blueprint.

[ ] **Task 0.4: Prioritize Therapeutic Candidates.**
    *   **Status:** Blocked by user review of the generated blueprint.
    *   **Action:** Analyze the campaign results to identify and rank the top candidates for each therapeutic modality.
    *   **Deliverable:** A finalized list for wet-lab synthesis.

**Phase I: Experimental Validation (The Proving Ground)**
**Objective:** To establish an undeniable, quantitative correlation between our platform's predictions and experimental outcomes.

[ ] **Task 1.1: Synthesize & Test Therapeutic Candidates.**
    *   **Status:** Blocked by Phase 0.
    *   **Action:** Synthesize the prioritized therapeutic candidates and test efficacy/safety in RUNX1-mutated cell lines.

[ ] **Task 1.2: In Vivo Efficacy Study.**
    *   **Status:** Blocked by Task 1.1.
    *   **Action:** Test the most promising candidates in a RUNX1-FPD mouse model.

**Phase II: Commercialization & Platform Supremacy**
**Objective:** To transform our powerful backend into an accessible, market-leading product.

[ ] **Task 2.1: Formalize Intellectual Property.**
    *   **Status:** Blocked by Phase I.
    *   **Action:** Engage with patent attorneys to file patents on the novel therapeutic designs.

[ ] **Task 2.2: Construct the "CrisPRO Studio" UI.**
    *   **Status:** Can begin in parallel.
    *   **Action:** Develop a secure, intuitive web interface (e.g., using Streamlit or React) as the front-end for the `CommandCenter` API.

[ ] **Task 2.3: Close the Data Flywheel.**
    *   **Status:** Blocked by Phase I.
    *   **Action:** Build the data ingestion pipeline to feed validation results back into our model repositories for continuous improvement.

Lessons
=======

* This metastasis modeling use-case maps our Evo2 endpoints to the 8 steps of metastasis, showing how our platform can predict and prevent cancer spread through targeted CRISPR interventions.
* This use-case aligns with our platform's evolution toward comprehensive therapeutic CRISPR design, adding evolutionary biology and cancer ecology as foundational elements for next-generation precision medicine.

**üîç FALLBACK MODE EXPLANATION COMPLETE! üîç**