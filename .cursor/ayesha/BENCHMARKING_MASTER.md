# AYESHA Benchmarking - Master Guide

**Last Updated**: January 27, 2025  
**Status**: ‚úÖ Ready for Single-Case Benchmarking | ‚ö†Ô∏è Multi-Case Pending

---

## üìã Benchmark Types Overview

**This document covers THREE distinct validation approaches:**

### 1. **AYESHA Benchmarks** (This Document's Focus)
- **Purpose**: Single-case validation for MBD4+TP53 HGSOC case
- **Type**: Consistency & alignment checks (not real outcome validation)
- **Scripts**: `benchmark_mbd4_tp53_accuracy.py`, `benchmark_clinical_validation.py`, `benchmark_brca_tp53_proxy.py`
- **Status**: ‚úÖ Ready to run

### 2. **SOTA Benchmarks** (Separate System Validation)
- **Purpose**: Multi-disease validation (MM, Ovarian, Melanoma) to verify system-wide fixes
- **Type**: Performance validation against known ground truth (pathway alignment, drug ranking)
- **Scripts**: `benchmark_sota_mm.py`, `benchmark_sota_ovarian.py`, `benchmark_sota_melanoma.py`
- **Status**: ‚úÖ Ready to run (all critical fixes complete)
- **See**: `.cursor/plans/sota-benchmarks-and-frontend-integration-aa7ca3bc.plan.md` for details

### 3. **Verification Layer** (NEW - January 2025)
- **Purpose**: Automated correctness checks for analysis outputs (deterministic validation)
- **Type**: Verification against authoritative sources (ClinVar, COSMIC, KEGG, Reactome, FDA, NCCN)
- **Scripts**: `verify_variant_classification.py`, `verify_pathway_mapping.py`, `verify_mechanism_vector.py`, `verify_mbd4_analysis.py`
- **Status**: ‚úÖ P0 tasks complete (4/8 scripts), P1 tasks pending
- **See**: `.cursor/ayesha/VERIFICATION_LAYER_PROGRESS.md` for details

**Key Differences**: 
- **AYESHA benchmarks** = Single rare case (MBD4+TP53) consistency validation
- **SOTA benchmarks** = Multi-disease (MM/Ovarian/Melanoma) performance validation
- **Verification layer** = Deterministic correctness checks (ClinVar, COSMIC, KEGG, Reactome, formulas)

**All three are important but serve different purposes:**
- **Benchmarks** = "Are our predictions accurate?" (requires ground truth)
- **Verification** = "Are our outputs correct?" (deterministic checks)

---

## üöÄ Quick Start

### Run Benchmarks NOW

```bash
cd oncology-coPilot/oncology-backend-minimal

# 1. Accuracy Benchmark (Automated Tests)
python3 scripts/benchmark_mbd4_tp53_accuracy.py

# 2. Clinical Validation (NCCN/FDA Comparison)
python3 scripts/benchmark_clinical_validation.py

# 3. BRCA+TP53 Proxy (Real Accuracy Validation)
python3 scripts/benchmark_brca_tp53_proxy.py
```

### Run Verification Layer (NEW)

```bash
cd oncology-coPilot/oncology-backend-minimal

# After running analysis, verify outputs:
python3 scripts/sae/verify_mbd4_analysis.py results/ayesha_analysis/ayesha_mbd4_tp53_analysis_*.json

# Or run individual verification checks:
python3 scripts/sae/verify_variant_classification.py <analysis.json> [api_base]
python3 scripts/sae/verify_pathway_mapping.py <analysis.json>
python3 scripts/sae/verify_mechanism_vector.py <analysis.json>
```

**Note**: Verification layer checks deterministic correctness (ClinVar, COSMIC, KEGG, Reactome, formulas), while benchmarks check predictive accuracy (requires ground truth).

### What Gets Tested

**5 Critical Accuracy Dimensions**:
1. **Pathway Accuracy** - Do pathway scores match biology?
2. **Drug Accuracy** - Do recommendations match NCCN/FDA?
3. **Mechanism Accuracy** - Do mechanism vectors match clinical mechanisms?
4. **Synthetic Lethality** - Do we correctly identify PARP sensitivity?
5. **Evidence Alignment** - Do evidence tiers match clinical evidence?

### Success Criteria

- **Overall Pass Rate**: ‚â•85%
- **Drug Accuracy**: ‚â•90% (critical for patient safety)
- **Pathway Accuracy**: ‚â•80%
- **Mechanism Vector**: ‚â•80%

---

## üéØ What We're Testing

### Current Benchmark: MBD4+TP53 (Consistency & Alignment)

**We're NOT testing accuracy against real patient outcomes** (MBD4+TP53 is too rare).

**We ARE testing**:
1. **Internal Consistency** - Does the system work as designed?
2. **Clinical Alignment** - Do recommendations match guidelines?
3. **Biological Soundness** - Do mechanisms match literature?

### Test Breakdown

#### Test 1: Pathway Accuracy
- **Expected**: DDR=1.0 (MBD4 frameshift ‚Üí complete loss), TP53=0.8 (hotspot ‚Üí high disruption)
- **Source**: Biological assumptions (frameshift = 1.0, hotspot = 0.8)
- **This is**: Internal consistency check (not real outcome validation)

#### Test 2: Drug Recommendations
- **Expected**: PARP #1-3, Platinum #4, efficacy ‚â•0.75
- **Source**: NCCN Guidelines for general HRD+ ovarian cancer
- **This is**: Alignment with general guidelines (not MBD4+TP53 specific)

#### Test 3: Mechanism Vectors
- **Expected**: DDR=1.4 (1.0 + 0.8√ó0.5)
- **Source**: Our conversion formula
- **This is**: Logic validation (not outcome validation)

---

## üìä Ground Truth: What Are We Actually Comparing Against?

### ‚ö†Ô∏è HONEST ANSWER: We Don't Have Real Ground Truth

#### What We're Currently Using (NOT Real Ground Truth):

1. **Biology-Based Expected Values** (Not Real Outcomes)
   ```python
   GROUND_TRUTH = {
       "expected_pathway_disruption": {
           "ddr": {"min": 0.9, "max": 1.0},  # MBD4 frameshift = complete loss
           "tp53": {"min": 0.7, "max": 0.9}  # TP53 hotspot = high disruption
       }
   }
   ```
   - **Source**: Our biological assumptions
   - **Problem**: These are **expected** values, not **validated** outcomes

2. **Clinical Guidelines** (General, Not Case-Specific)
   - **Source**: NCCN Guidelines for **general HRD+ ovarian cancer**
   - **Problem**: Guidelines are for **BRCA1/BRCA2 HRD+**, not specifically **MBD4+TP53**

3. **Literature Knowledge** (Pathway Biology, Not Outcomes)
   - **Source**: Published literature on MBD4 biology
   - **Problem**: Literature describes **mechanism**, not **actual patient outcomes**

### ‚ùå What We're NOT Comparing Against:

1. **Real Patient Outcomes**: No actual MBD4+TP53 patients with known drug responses
2. **Gold Standard System**: No other system's predictions to compare against
3. **Published Case Studies**: No published MBD4+TP53 HGSOC cases with outcomes
4. **Clinical Trial Results**: No trial data specifically for MBD4+TP53 combination
5. **Expert Consensus**: No panel of oncologists who reviewed this specific case

### ‚úÖ What We CAN Benchmark Now:

1. **Internal Consistency** (Works)
   - Pathway scores match variant types (frameshift = 1.0, hotspot = 0.8)
   - Mechanism vectors match pathway scores (DDR = 1.0 + 0.8√ó0.5 = 1.4)
   - Drug recommendations match pathway disruption (DDR high ‚Üí PARP)
   - **Value**: Validates system logic is correct

2. **Clinical Guideline Alignment** (Works)
   - PARP inhibitors recommended (NCCN Category 1 for HRD+)
   - Platinum recommended (Standard of care)
   - Evidence tiers match guideline strength
   - **Value**: Validates recommendations align with clinical standards

3. **Biological Mechanism** (Works)
   - MBD4 ‚Üí BER deficiency (literature-supported)
   - TP53 ‚Üí Checkpoint bypass (literature-supported)
   - Combined ‚Üí Synthetic lethality (mechanism-supported)
   - **Value**: Validates biological reasoning is sound

### üö® What We CANNOT Benchmark:

- ‚ùå **Real-World Accuracy**: No real patient outcomes to compare against
- ‚ùå **Predictive Performance**: No way to measure accuracy of efficacy scores
- ‚ùå **Comparative Performance**: No other system to compare against

---

## üéØ Better Ground Truth Options

**‚ö†Ô∏è Critical Caveat**: All options below have significant limitations. We don't have perfect ground truth for MBD4+TP53. These are the best available alternatives we've identified, but each has trade-offs and unknowns. We need to:

1. **Acknowledge what we don't know**: We can't directly validate MBD4+TP53 predictions against real patient outcomes (they don't exist)
2. **Be cautious about claims**: Any validation approach is imperfect and has limitations
3. **Validate incrementally**: Test each approach and see what we learn, rather than assuming it will work
4. **Document limitations**: Be explicit about what each approach can and cannot validate

**What We're Trying to Do**: Find the best available proxy for real outcome validation, acknowledging it's not perfect.

### Option 1: BRCA+TP53 Proxy

**Rationale**: BRCA+TP53 is biologically similar to MBD4+TP53 (both involve DNA repair deficiency and HRD+), and has published RCT data we can use as a proxy.

#### ‚úÖ Potential Benefits:

1. **Published RCT Data Available**:
   - SOLO-2: Olaparib response rate = 65% (BRCA+TP53 subset)
   - PAOLA-1: Olaparib+bevacizumab response rate = 64% (HRD+)
   - PRIMA: Niraparib response rate = 57% (HRD+)
   - ARIEL3: Rucaparib response rate = 64% (BRCA+TP53 subset)
   - **Note**: This is published patient outcome data, not assumptions

2. **Could Validate Against Real Outcomes** (if correlation is good):
   - Could measure: "Do our efficacy scores correlate with published response rates?"
   - Could measure: "Do we recommend the same drugs that worked in trials?"
   - **Note**: This would be accuracy validation, not just consistency

3. **Infrastructure Exists**:
   - ‚úÖ `benchmark_brca_tp53_proxy.py` script exists
   - **Note**: Still need to verify it works correctly and extract response rates accurately

#### ‚ùå Limitations & Unknowns:

1. **Not Exactly MBD4+TP53**:
   - BRCA1/BRCA2 ‚Üí HRD (homologous recombination deficiency)
   - MBD4 ‚Üí BER (base excision repair deficiency)
   - **Different pathways** (though both may lead to PARP sensitivity)
   - **Unknown**: How well does BRCA+TP53 proxy for MBD4+TP53? We don't have data to answer this.

2. **Timeline Uncertainty**: 
   - Estimated 2-3 days to extract response rates from published RCTs
   - **Unknown**: How long to map RCT data to our test cases accurately

3. **Validation Status**:
   - **Unknown**: Will our efficacy scores actually correlate with published response rates?
   - **Unknown**: What correlation threshold indicates "good enough"?
   - **Unknown**: Does this validate accuracy for MBD4+TP53, or just for BRCA+TP53?

**Assessment**: This appears to be the best available option for real outcome validation, but we need to acknowledge it's a proxy, not direct validation.

### Option 2: Expert Panel Review

**Rationale**: Get clinical consensus from oncologists on the AYESHA case to establish expert-based ground truth.

**What We Would Need**:
- 5+ oncologists to review the case
- Consensus on recommended drugs, pathways, mechanisms
- Use consensus as ground truth

**Timeline**: Estimated 1-2 weeks (to organize panel and get reviews)

#### ‚úÖ Potential Benefits:
- Clinical expert consensus (not just biology assumptions)
- Could validate against real clinical decision-making

#### ‚ùå Limitations & Unknowns:
- **Time-consuming**: 1-2 weeks to organize
- **Cost**: May require compensation for expert time
- **Consensus uncertainty**: What if experts disagree?
- **Validation status**: Is expert consensus actually "ground truth" or just another form of validation?
- **Unknown**: How do we handle disagreements between experts?

**Assessment**: Potentially valuable but requires significant time and coordination. Not immediately actionable.

### Option 3: Synthetic Test Cases

**Rationale**: Create test cases with known biology to validate system consistency across pathways.

#### ‚úÖ Potential Benefits:

1. **Fast to Create**: Estimated 1 day to create test cases
2. **Comprehensive Coverage**: Could cover many pathways (DDR, PI3K, MAPK, VEGF)
3. **Internal Consistency Check**: Could validate system logic works across different pathways

#### ‚ùå Limitations & Unknowns:

1. **Still Based on Assumptions**:
   - Expected values would be based on variant types (e.g., frameshift = 1.0, hotspot = 0.8)
   - **Same limitation as MBD4+TP53 benchmark**: Not real outcomes, just biological assumptions
   - **Unknown**: Are our assumptions about variant types ‚Üí pathway scores actually correct?

2. **Doesn't Validate Real-World Accuracy**:
   - Can't measure: "Do our predictions match real patient outcomes?"
   - Can only measure: "Does the system work as designed?" (consistency)
   - **This is a consistency check, not accuracy validation**

3. **Validation Status**:
   - **Unknown**: Does consistency across pathways mean the system is accurate?
   - **Unknown**: How do we know our expected values are correct?

**Assessment**: Useful for internal consistency validation, but doesn't address the core question of real-world accuracy. Fast to implement but limited value for accuracy validation.

### üîç Comparison of Options

| Criteria | Option 1: BRCA+TP53 | Option 2: Expert Panel | Option 3: Synthetic |
|----------|---------------------|------------------------|---------------------|
| **Real Outcomes?** | ‚úÖ Yes (published RCTs) | ‚ö†Ô∏è Expert consensus (not outcomes) | ‚ùå No (assumptions) |
| **Accuracy Validation?** | ‚ö†Ô∏è Could validate (if correlation good) | ‚ö†Ô∏è Could validate (if consensus reliable) | ‚ùå No (consistency only) |
| **Timeline** | 2-3 days (estimated) | 1-2 weeks | 1 day |
| **Clinical Relevance** | ‚úÖ High (real patient data) | ‚úÖ High (expert consensus) | ‚ö†Ô∏è Medium (biology only) |
| **Infrastructure** | ‚úÖ Script exists | ‚ùå Need to organize | ‚ö†Ô∏è Need to create |
| **Limitations** | Proxy, not direct | Time/cost, consensus issues | Assumptions, not outcomes |

**‚ö†Ô∏è Important Note**: None of these options provide perfect ground truth. Each has trade-offs and limitations.

### üí° Potential Approach: Hybrid (with caveats)

**‚ö†Ô∏è Disclaimer**: This is a suggested approach, not a definitive recommendation. We need to validate each step before proceeding.

#### Phase 1: Try BRCA+TP53 Proxy (2-3 days estimated)

**Rationale**: This appears to be the best available option for real outcome validation, though it's a proxy.

**What We Would Attempt to Validate**:
- Do our efficacy scores correlate with published response rates? (if correlation is good)
- Do we recommend the same drugs that worked in trials? (if recommendations match)
- **Note**: This would validate accuracy for BRCA+TP53, not directly for MBD4+TP53

**Deliverable**: Correlation analysis between our predictions and published RCT outcomes (if correlation exists)

**Caveats**:
- We don't know if correlation will be good
- We don't know if this validates MBD4+TP53 or just BRCA+TP53
- Need to verify response rate extraction is accurate

#### Phase 2: Consider Synthetic Test Cases (1 day, if time permits)

**Rationale**: Fast internal consistency check, though limited value for accuracy validation.

**What We Would Attempt to Validate**:
- Does system work for DDR, PI3K, MAPK, VEGF pathways? (consistency check)
- Are pathway scores consistent across variant types? (consistency check)
- **Note**: This is consistency, not accuracy validation

**Deliverable**: Multi-pathway consistency report (if consistent)

**Caveats**:
- Doesn't validate real-world accuracy
- Based on assumptions about variant types
- Limited value for accuracy validation

#### Phase 3: Analyze Results (1 day)

**What We Would Do**:
- Compare BRCA+TP53 correlation results (if available)
- Review synthetic consistency results (if available)
- Identify gaps and limitations
- **Note**: May not provide definitive answers

**Caveats**:
- Results may be inconclusive
- May need additional validation approaches
- May reveal we need expert panel review after all

---

## üìä Benchmark Comparison Table

| Benchmark Type | Ground Truth | Real Outcomes? | Accuracy? | Purpose |
|----------------|--------------|---------------|-----------|---------|
| **MBD4+TP53** | Biology + Guidelines | ‚ùå No | ‚ùå No (consistency only) | Internal consistency check |
| **BRCA+TP53 Proxy** | Published RCTs | ‚úÖ Yes | ‚úÖ Yes (real accuracy) | Real accuracy validation |

### Bottom Line

**MBD4+TP53 Benchmark**:
- ‚úÖ Tests: Internal consistency, clinical alignment, biological soundness
- ‚ùå Does NOT test: Real-world accuracy (no outcome data)
- **This is**: "Consistency & Alignment Benchmark"

**BRCA+TP53 Proxy Benchmark**:
- ‚úÖ Tests: Real-world accuracy (published RCT outcomes)
- ‚úÖ Tests: Predictive performance (efficacy vs. response rates)
- ‚úÖ Tests: Clinical alignment (same drugs that worked in trials)
- **This is**: "Real Accuracy Benchmark"

**‚ö†Ô∏è Important Note**: BRCA+TP53 proxy appears to be the best available option for real outcome validation, but it's still a proxy. We need to:
1. Run the benchmark and see if correlation exists
2. Validate that the correlation is meaningful
3. Acknowledge that this validates BRCA+TP53, not directly MBD4+TP53
4. Consider additional validation approaches if results are inconclusive

---

## üìã Current Status & Dataset

### ‚úÖ What We Have

#### 1. Ground Truth Data
- **Location**: `scripts/benchmark_mbd4_tp53_accuracy.py` (lines 33-89)
- **Case**: AYESHA-001 (MBD4+TP53 HGSOC)
- **Expected Values**:
  - Pathway: DDR=1.0, TP53=0.8
  - Drugs: PARP #1-3, Platinum #4
  - Mechanism Vector: DDR=1.4 (1.0 + 0.8√ó0.5)
  - Synthetic Lethality: PARP/platinum

#### 2. Test Infrastructure
- ‚úÖ **Benchmark Script**: `benchmark_mbd4_tp53_accuracy.py` (633 lines)
- ‚úÖ **Clinical Validation**: `benchmark_clinical_validation.py` (215 lines)
- ‚úÖ **BRCA Proxy**: `benchmark_brca_tp53_proxy.py` (already created)
- ‚úÖ **Pytest Tests**: `tests/test_mbd4_tp53_analysis.py` (340 lines)
- ‚úÖ **Backend Running**: Verified healthy

#### 3. Historical Results
- ‚úÖ **5 AYESHA Analysis Runs**: `results/ayesha_analysis/`
- ‚úÖ **Latest**: `ayesha_mbd4_tp53_analysis_20251127_013200.json` (6,839 lines)

#### 4. Clinical Evidence Database
- ‚úÖ **NCCN Guidelines**: PARP inhibitors Category 1
- ‚úÖ **FDA Labels**: Olaparib, niraparib, rucaparib approved
- ‚úÖ **Published RCTs**: SOLO-2, PAOLA-1, PRIMA, NOVA, ARIEL3
- ‚úÖ **Literature**: MBD4 BER pathway, TP53 checkpoint pathway

### ‚ùå What We're Missing

#### 1. Multi-Case Dataset
- **Current**: Only 1 test case (MBD4+TP53)
- **Needed**: 10+ HGSOC cases with known outcomes
- **Why**: Single case can't validate generalizability

#### 2. Ground Truth for Other Cases
- **Current**: Ground truth only for AYESHA
- **Needed**: Ground truth for multiple variant combinations
- **Examples Needed**:
  - BRCA1/BRCA2 + TP53 (common HRD+)
  - PIK3CA + TP53 (PI3K pathway)
  - KRAS + TP53 (MAPK pathway)
  - Multiple TP53 hotspots (R175H, R248Q, R273H)

#### 3. Real Patient Outcomes
- **Current**: Expected values based on biology
- **Needed**: Actual clinical outcomes (response rates, PFS)
- **Why**: Validate predictions match real-world results

### üöÄ Can We Run Benchmarks NOW?

#### ‚úÖ YES - Single Case Benchmarking

**What Works**:
```bash
# Run accuracy benchmark (AYESHA case only)
python3 scripts/benchmark_mbd4_tp53_accuracy.py

# Run clinical validation (NCCN/FDA comparison)
python3 scripts/benchmark_clinical_validation.py

# Run BRCA+TP53 proxy (real accuracy)
python3 scripts/benchmark_brca_tp53_proxy.py
```

**What It Tests**:
- ‚úÖ Pathway accuracy (DDR=1.0, TP53=0.8)
- ‚úÖ Drug recommendations (PARP #1-3, Platinum #4)
- ‚úÖ Mechanism vectors (DDR=1.4)
- ‚úÖ Synthetic lethality (PARP/platinum)
- ‚úÖ Evidence alignment (supported/consider tiers)

**Limitation**: Only validates ONE case (MBD4+TP53)

---

## üìà Example Output

```
================================================================================
AYESHA MBD4+TP53 ACCURACY BENCHMARK RESULTS
================================================================================

Total Tests: 15
Passed: 14 (93.3%)
Failed: 1 (6.7%)
Average Score: 0.912

--------------------------------------------------------------------------------
‚úÖ PASS | Pathway Accuracy - DDR (MBD4)
      Score: 1.000
      Expected: 0.9-1.0
      Actual: 1.0000

‚úÖ PASS | Pathway Accuracy - TP53 (R175H)
      Score: 0.875
      Expected: 0.7-0.9
      Actual: 0.8000

‚úÖ PASS | Drug Accuracy - olaparib Efficacy
      Score: 0.800
      Expected: >= 0.75
      Actual: 0.800

‚úÖ PASS | Drug Accuracy - PARP in Top 3
      Score: 1.000
      Expected: PARP inhibitor in top 3
      Actual: Top 3: ['olaparib', 'niraparib', 'rucaparib']

‚úÖ PASS | Mechanism Vector - DDR
      Score: 1.000
      Expected: 1.2-1.5
      Actual: 1.4000

‚ùå FAIL | Evidence Alignment - PARP Evidence Tier
      Score: 0.500
      Expected: supported or consider
      Actual: insufficient
      Error: PARP should have strong evidence (HRD+), got: insufficient

================================================================================
```

---

## üéØ How to Interpret Results

### ‚úÖ All Tests Pass (‚â•85%)
- **Status**: Production ready
- **Action**: Ship it!

### ‚ö†Ô∏è Some Tests Fail (70-85%)
- **Status**: Needs improvement
- **Action**: Fix failing tests, re-run

### ‚ùå Many Tests Fail (<70%)
- **Status**: Not production ready
- **Action**: Debug issues, check ground truth assumptions

---

## üîß Adding More Test Cases

### 1. Add to Ground Truth

Edit `benchmark_mbd4_tp53_accuracy.py`:

```python
GROUND_TRUTH = {
    "expected_drugs": {
        "tier1": [
            {"name": "new_drug", "min_efficacy": 0.70, "expected_rank": 5}
        ]
    }
}
```

### 2. Add New Test

```python
async def test_new_feature(self):
    """Test new feature"""
    # Your test logic
    self.add_result("New Test", passed, score, expected, actual)
```

### 3. Register Test

```python
async def run_all_tests(self):
    await self.test_new_feature()  # Add here
```

---

## üìÅ Output Files

### Benchmark Results
- **Location**: `results/benchmarks/ayesha_accuracy_benchmark_<timestamp>.json`
- **Format**: JSON with test results, scores, expected vs. actual

### Clinical Validation
- **Location**: Console output (redirect to file if needed)
- **Format**: Human-readable comparison with clinical evidence

---

## üö¶ CI/CD Integration

### Run on Every Commit

```bash
# In CI/CD pipeline
python3 scripts/benchmark_mbd4_tp53_accuracy.py
# Exit code: 0 if pass_rate >= 0.8, else 1
```

### Regression Testing

```bash
# Compare against baseline
python3 scripts/benchmark_mbd4_tp53_accuracy.py > current.txt
diff baseline.txt current.txt
```

---

## üéì Key Metrics

### Accuracy Metrics
- **Precision**: Correct positives / Total positives
- **Recall**: Correct positives / Total expected
- **F1 Score**: Harmonic mean of precision and recall

### Clinical Metrics
- **NCCN Alignment**: % matching NCCN Category 1
- **FDA Alignment**: % matching FDA-approved indications
- **Evidence Strength**: % with RCT evidence

---

## ‚ö†Ô∏è Known Limitations

1. **Ground Truth Assumptions**: Based on variant type, may not capture all nuances
2. **Clinical Evidence**: Based on published guidelines (may lag research)
3. **Rare Combinations**: MBD4+TP53 has limited published evidence

---

## üîÆ Future Enhancements

1. **Real Patient Validation**: Compare with actual clinical outcomes
2. **Multi-Case Benchmarking**: Test on 10+ HGSOC cases
3. **Trial Matching Validation**: Compare mechanism fit with actual enrollment
4. **SAE Comparison**: When True SAE available, compare accuracy

---

## üìã Quick Checklist

### Ready to Run NOW:
- [x] Ground truth defined (AYESHA case)
- [x] Benchmark scripts created
- [x] Backend running
- [x] Historical results available
- [x] Clinical evidence database
- [x] BRCA+TP53 proxy script ready

### Need for Multi-Case:
- [ ] 10+ test cases with ground truth
- [ ] Automated test case runner
- [ ] Aggregated results dashboard
- [ ] Comparison against baseline

---

## üö¶ Current Status Summary

**Single-Case Benchmarking**: ‚úÖ **READY** (for MBD4+TP53 case only)  
**Multi-Case Benchmarking**: ‚ö†Ô∏è **NOT READY** (need dataset)

**Suggested Next Steps** (subject to validation):
- Consider running single-case benchmark to establish baseline
- Consider creating multi-case dataset when ready
- **Note**: All benchmarks have limitations and should be interpreted cautiously

---

## üí° Key Insights (with caveats)

1. **MBD4+TP53 Benchmark = Consistency & Alignment Check**
   - **What it does**: Validates system works as designed (internal consistency)
   - **What it doesn't do**: Does NOT validate real-world accuracy
   - **Limitation**: Based on biological assumptions, not real outcomes

2. **BRCA+TP53 Proxy = Potential Accuracy Validation** (if correlation exists)
   - **What it could do**: Could validate against published RCT outcomes (if we see good correlation)
   - **What it might measure**: Could measure predictive performance (if correlation is meaningful)
   - **Limitations**: 
     - It's a proxy, not direct validation of MBD4+TP53
     - We don't know if correlation will be good
     - We don't know what correlation threshold indicates "good enough"

3. **Multiple Approaches May Be Needed**:
   - BRCA+TP53 proxy: Could tell us "Are we accurate for similar cases?" (if correlation good)
   - Synthetic test cases: Could tell us "Do we work across pathways?" (consistency check)
   - **Note**: Neither provides perfect validation, but together they might provide useful information

4. **BRCA+TP53 Proxy Appears More Valuable** (but needs validation):
   - **Why**: It uses real outcome data, not assumptions
   - **But**: We need to actually run it and see if correlation exists
   - **And**: We need to acknowledge it's a proxy, not direct validation

---

**Quick Command Reference**:

```bash
# Run accuracy benchmark
python3 scripts/benchmark_mbd4_tp53_accuracy.py

# Run clinical validation
python3 scripts/benchmark_clinical_validation.py

# Run BRCA+TP53 proxy (real accuracy)
python3 scripts/benchmark_brca_tp53_proxy.py

# All three (if you want)
python3 scripts/benchmark_mbd4_tp53_accuracy.py && \
python3 scripts/benchmark_clinical_validation.py && \
python3 scripts/benchmark_brca_tp53_proxy.py
```

---

---

## üîß SOTA Benchmarks Context (For Reference)

### What Are SOTA Benchmarks?

**SOTA (State-of-the-Art) Benchmarks** are separate from AYESHA benchmarks. They validate system-wide performance across multiple diseases after critical bug fixes.

### SOTA Benchmark Scripts

**Location**: `oncology-coPilot/oncology-backend-minimal/scripts/`

1. **MM Benchmark** (`benchmark_sota_mm.py`)
   - **Target**: >80% pathway alignment accuracy
   - **Tests**: MAPK pathway variants (KRAS, BRAF, NRAS)
   - **Previous**: 40% (broken) ‚Üí **Target**: >80% (fixed)
   - **Status**: ‚úÖ Script ready, pending execution

2. **Ovarian Benchmark** (`benchmark_sota_ovarian.py`)
   - **Target**: AUROC >0.75 (stretch), >0.65 (minimum)
   - **Tests**: 1k dataset (500 sensitive, 500 resistant variants)
   - **Previous**: 0.500 (random) ‚Üí **Target**: >0.75 (fixed)
   - **Status**: ‚úÖ Script ready, pending execution

3. **Melanoma Benchmark** (`benchmark_sota_melanoma.py`)
   - **Target**: >90% drug ranking accuracy
   - **Tests**: BRAF V600E, NRAS Q61K (known driver mutations)
   - **Previous**: 50% ‚Üí **Target**: >90% (fixed)
   - **Status**: ‚úÖ Script ready, pending execution

### Critical Fixes Applied (January 2025)

**All fixes verified and ready for SOTA benchmarks:**

1. ‚úÖ **Pathway Normalization** - Fixed range from (1e-6 to 1e-4) to (0 to 0.005)
2. ‚úÖ **Tier Computation** - Fixed to use raw `s_path` instead of normalized `path_pct`
3. ‚úÖ **Tier Threshold** - Adjusted from 0.05 to 0.001 for new pathway score range
4. ‚úÖ **Sporadic Gates** - Fixed to only apply when tumor context actually provided

**Files Modified**:
- `api/services/efficacy_orchestrator/drug_scorer.py` (lines 48-55, 139)
- `api/services/confidence/tier_computation.py` (line 61)
- `api/services/efficacy_orchestrator/orchestrator.py` (lines 225-228)

### Running SOTA Benchmarks

**Prerequisites**:
1. Backend server running (`uvicorn api.main:app --host 0.0.0.0 --port 8000`)
2. Server accessible at `http://127.0.0.1:8000`

**Commands**:
```bash
cd oncology-coPilot/oncology-backend-minimal

# Run all three SOTA benchmarks
python3 scripts/benchmark_sota_mm.py
python3 scripts/benchmark_sota_ovarian.py
python3 scripts/benchmark_sota_melanoma.py
```

**Expected Results** (after fixes):
- MM: >80% pathway alignment (was 40%)
- Ovarian: AUROC >0.75 (was 0.500)
- Melanoma: >90% drug ranking (was 50%)

### Relationship to AYESHA Benchmarks

**SOTA benchmarks** validate system-wide fixes work across diseases.  
**AYESHA benchmarks** validate specific case (MBD4+TP53) consistency.

**Both should pass** for production readiness:
- ‚úÖ SOTA benchmarks ‚Üí System works correctly across diseases
- ‚úÖ AYESHA benchmarks ‚Üí System works correctly for rare case

**See Also**:
- `.cursor/plans/sota-benchmarks-and-frontend-integration-aa7ca3bc.plan.md` - Full SOTA plan
- `oncology-coPilot/oncology-backend-minimal/PRE_BENCHMARK_AUDIT.md` - System audit

---

---

## üîç Verification Layer Context (For Other Agents)

### What Is the Verification Layer?

**Verification Layer** is a separate validation system that checks **deterministic correctness** of analysis outputs, as opposed to **predictive accuracy** (which benchmarks measure).

**Key Distinction**:
- **Benchmarks** = "Are our predictions accurate?" (requires ground truth outcomes)
- **Verification** = "Are our outputs correct?" (deterministic checks against authoritative sources)

### Verification vs. Benchmarking

| Aspect | Verification Layer | Benchmarks |
|--------|-------------------|------------|
| **Purpose** | Check correctness of outputs | Check accuracy of predictions |
| **Ground Truth** | Authoritative databases (ClinVar, COSMIC, KEGG, Reactome) | Clinical outcomes or known biology |
| **Confidence** | 90-100% (deterministic) | 70-90% (predictive) |
| **What It Validates** | Variant classification, pathway mapping, formulas, structure | Drug efficacy, pathway scores, mechanism vectors |
| **When to Use** | After every analysis run | For validation studies |

### Available Verification Scripts

**Location**: `scripts/sae/`

1. **`verify_variant_classification.py`** ‚úÖ
   - Checks: ClinVar classification, COSMIC hotspots, Evo2 delta scores
   - Validates: Variant impact predictions against authoritative sources
   - Output: `*_variant_verification.json`

2. **`verify_pathway_mapping.py`** ‚úÖ
   - Checks: KEGG pathways, Reactome pathways, DNA repair formula, TCGA weights
   - Validates: Gene‚Üípathway mappings and formula correctness
   - Output: `*_pathway_verification.json`

3. **`verify_mechanism_vector.py`** ‚úÖ
   - Checks: Vector structure (7D), pathway mapping, IO eligibility
   - Validates: Mechanism vector correctness and pathway‚Üívector mapping
   - Output: `*_mechanism_vector_verification.json`

4. **`verify_mbd4_analysis.py`** ‚úÖ
   - Runs all verification scripts and aggregates results
   - Computes overall pass rate
   - Output: `*_verification.json`

**Pending (P1 Tasks)**:
- `verify_functional_annotation.py` (UniProt, insights bundle)
- `verify_eligibility_io.py` (FDA labels, NCCN guidelines)
- `verify_consistency.py` (pathway scores, variant annotations)

### How Verification Complements Benchmarks

**Verification Layer** answers:
- ‚úÖ "Is the variant correctly classified as Pathogenic?" (ClinVar check)
- ‚úÖ "Is MBD4 correctly mapped to DDR pathway?" (KEGG/Reactome check)
- ‚úÖ "Is the DNA repair formula computed correctly?" (Formula check)
- ‚úÖ "Is the mechanism vector structure correct?" (Structure check)

**Benchmarks** answer:
- ‚úÖ "Do our pathway scores match expected biology?" (Pathway accuracy)
- ‚úÖ "Do our drug recommendations match NCCN/FDA?" (Drug accuracy)
- ‚úÖ "Do our efficacy scores correlate with clinical outcomes?" (Accuracy validation)

**Together**: Verification ensures correctness, benchmarks ensure accuracy.

### Integration with Analysis Pipeline

**Recommended Workflow**:

```bash
# 1. Run analysis
python3 scripts/sae/run_mbd4_tp53_analysis.py

# 2. Verify outputs (deterministic checks)
python3 scripts/sae/verify_mbd4_analysis.py results/ayesha_analysis/ayesha_mbd4_tp53_analysis_*.json

# 3. Run benchmarks (accuracy validation)
python3 scripts/benchmark_mbd4_tp53_accuracy.py
python3 scripts/benchmark_clinical_validation.py
```

**Verification should pass before running benchmarks** (ensures outputs are correct before validating accuracy).

### Verification Coverage

**What We Can Verify Now** (P0 Complete):
- ‚úÖ Variant classification (ClinVar, COSMIC, Evo2)
- ‚úÖ Pathway mapping (KEGG, Reactome)
- ‚úÖ DNA repair capacity formula
- ‚úÖ Mechanism vector structure and mapping
- ‚úÖ TCGA pathway weights

**What We Still Need** (P1 Pending):
- ‚è∏Ô∏è Functional annotation (UniProt, insights bundle)
- ‚è∏Ô∏è Eligibility & IO (FDA, NCCN)
- ‚è∏Ô∏è Consistency checks (pathway scores, variant annotations)

### References

- **Verification Layer Plan**: `.cursor/ayesha/MBD4_TP53_VERIFICATION_LAYER_IMPLEMENTATION_PLAN.md`
- **Verification Progress**: `.cursor/ayesha/VERIFICATION_LAYER_PROGRESS.md`
- **Critical Analysis**: `.cursor/ayesha/MBD4_TP53_ANSWERS_CRITICAL_ANALYSIS.md`

---

**Last Updated**: January 27, 2025  
**Status**: ‚úÖ Ready to Use

