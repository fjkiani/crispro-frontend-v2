# üéØ OVARIAN CANCER PIVOT: EXECUTION PLAN

**Date**: 2025-01-29  
**Status**: üöÄ **READY TO EXECUTE** - Pivot from BRCA to Ovarian  
**Focus**: AUROC 0.783 result (validated, publication-ready)

---

## üìä CURRENT STATUS

### **What We Have**:
- ‚úÖ **Validated Result**: AUROC 0.783 ¬± 0.100 (5-fold CV, 149 TCGA-OV patients)
- ‚úÖ **Method**: TRUE SAE (29 features: 9 DDR diamonds + 20 top features)
- ‚úÖ **Outcome**: Platinum resistance prediction (sensitive vs resistant/refractory)
- ‚úÖ **Baseline Comparison**: PROXY SAE (gene-level) AUROC 0.628 ¬± 0.119
- ‚úÖ **Improvement**: +15.5 percentage points (0.783 vs 0.628)
- ‚úÖ **Biological Validation**: All 9 diamond features map to DDR pathway
- ‚úÖ **Statistical Significance**: DDR_bin p = 0.0020, Cohen's d = 0.642

### **What's Missing**:
- ‚ö†Ô∏è **Nested Cross-Validation**: Current validation uses 5-fold CV, but feature selection may have leakage
- ‚ö†Ô∏è **External Validation**: Only TCGA-OV validated, no independent cohort
- ‚ö†Ô∏è **Manuscript**: Draft exists but needs updates for nested CV
- ‚ö†Ô∏è **Figures**: Need publication-quality figures (ROC curves, pathway maps)
- ‚ö†Ô∏è **Supplementary Materials**: Need detailed methods, feature lists, code availability

---

## üéØ PUBLICATION GOALS

### **Target Journal**: Clinical Cancer Research / JCO Precision Oncology

### **Key Claims**:
1. **Primary**: "SAE features achieve AUROC 0.783 for platinum resistance prediction, outperforming gene-level markers (AUROC 0.628) by 15.5 percentage points"
2. **Biological**: "All 9 resistance-elevated features map coherently to DDR pathway"
3. **Clinical**: "DDR_bin scores significantly higher in resistant patients (p = 0.0020)"

---

## üìã EXECUTION PLAN: 10 DELIVERABLES

### **PHASE 1: VALIDATION ENHANCEMENT (Week 1)**

#### **Deliverable 1.1: Nested Cross-Validation** ‚ö†Ô∏è **CRITICAL**
**Timeline**: 2 days  
**Priority**: üî• **HIGHEST** (Prevents data leakage)

**Task**: Re-run validation with nested CV to ensure no data leakage
- **Outer CV**: 5-fold (for final performance estimate)
- **Inner CV**: 5-fold (for feature selection within each outer fold)
- **Feature Selection**: FDR-corrected p-values, within inner CV only
- **Expected Result**: AUROC 0.75-0.78 (slight drop from 0.783 if leakage existed)

**Script**: `scripts/sae/validate_ov_nested_cv.py`

**üö® DATA LEAKAGE PREVENTION CHECKLIST**:

1. **Feature Selection Isolation**:
   - ‚úÖ Features selected ONLY on training fold (never on test fold)
   - ‚úÖ Statistical tests (p-values, Cohen's d) computed ONLY on training data
   - ‚úÖ Feature ranking done independently per outer fold
   - ‚ùå **FORBIDDEN**: Selecting features on full dataset before CV

2. **Preprocessing Isolation**:
   - ‚úÖ Normalization/scaling fit ONLY on training fold
   - ‚úÖ Mean/std computed from training, applied to test
   - ‚úÖ Missing value imputation fit on training only
   - ‚ùå **FORBIDDEN**: Computing statistics on full dataset

3. **Model Training Isolation**:
   - ‚úÖ Model trained ONLY on training fold
   - ‚úÖ Hyperparameters tuned ONLY on inner CV (within training fold)
   - ‚úÖ Class weights computed from training fold only
   - ‚ùå **FORBIDDEN**: Training on test data or using test labels

4. **Evaluation Isolation**:
   - ‚úÖ Test fold used ONLY for final evaluation
   - ‚úÖ No information from test fold used in training
   - ‚úÖ Predictions made ONLY on test fold
   - ‚ùå **FORBIDDEN**: Using test predictions to select features

5. **Verification Checks**:
   - ‚úÖ Assert: No test patient IDs in training fold
   - ‚úÖ Assert: Feature selection count matches training size
   - ‚úÖ Assert: Model coefficients match training fold size
   - ‚úÖ Log: Which features selected in each fold (for stability check)

**Implementation Template**:
```python
def validate_ov_nested_cv():
    """
    Nested CV validation for ovarian SAE model.
    
    CRITICAL: No data leakage allowed!
    - Feature selection: ONLY on training fold
    - Preprocessing: Fit ONLY on training fold
    - Model training: ONLY on training fold
    - Evaluation: ONLY on test fold
    """
    from sklearn.model_selection import StratifiedKFold
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import roc_auc_score
    from statsmodels.stats.multitest import multipletests
    import numpy as np
    
    # Load data
    X, y, patient_ids = load_ov_sae_data()  # [n_patients, n_features]
    
    # Outer CV: 5-fold
    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    outer_scores = []
    feature_selection_log = []  # Track features selected per fold
    
    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):
        print(f"Outer fold {outer_fold + 1}/5")
        
        # ‚úÖ ISOLATION CHECK: Verify no overlap
        assert len(set(train_idx) & set(test_idx)) == 0, "TRAIN/TEST OVERLAP!"
        
        X_train_outer = X[train_idx]
        X_test_outer = X[test_idx]
        y_train_outer = y[train_idx]
        y_test_outer = y[test_idx]
        train_ids = [patient_ids[i] for i in train_idx]
        test_ids = [patient_ids[i] for i in test_idx]
        
        # ‚úÖ ISOLATION CHECK: Verify no patient ID overlap
        assert len(set(train_ids) & set(test_ids)) == 0, "PATIENT ID OVERLAP!"
        
        # Inner CV: Feature selection on training fold ONLY
        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        feature_scores = np.zeros(X.shape[1])  # Track feature significance
        
        for inner_fold, (train_inner_idx, val_inner_idx) in enumerate(
            inner_cv.split(X_train_outer, y_train_outer)
        ):
            # ‚úÖ ISOLATION CHECK: Inner CV uses only training fold
            X_train_inner = X_train_outer[train_inner_idx]
            X_val_inner = X_train_outer[val_inner_idx]
            y_train_inner = y_train_outer[train_inner_idx]
            y_val_inner = y_train_outer[val_inner_idx]
            
            # Feature selection: Compute statistics on inner training ONLY
            for feat_idx in range(X.shape[1]):
                # ‚úÖ Statistical test on inner training fold ONLY
                from scipy.stats import mannwhitneyu
                resistant_vals = X_train_inner[y_train_inner == 1, feat_idx]
                sensitive_vals = X_train_inner[y_train_inner == 0, feat_idx]
                
                if len(resistant_vals) > 0 and len(sensitive_vals) > 0:
                    u_stat, p_val = mannwhitneyu(
                        resistant_vals, sensitive_vals, alternative='greater'
                    )
                    # Track p-values (will aggregate across inner folds)
                    feature_scores[feat_idx] += -np.log10(p_val + 1e-10)
        
        # Aggregate feature scores across inner folds
        feature_scores = feature_scores / inner_cv.n_splits
        
        # FDR correction on training fold features ONLY
        p_values = 10 ** (-feature_scores)
        _, p_corrected, _, _ = multipletests(
            p_values, alpha=0.05, method='fdr_bh'
        )
        
        # Select features: FDR-corrected p < 0.05
        selected_features = np.where(p_corrected < 0.05)[0]
        
        # ‚úÖ LOG: Track selected features
        feature_selection_log.append({
            'outer_fold': outer_fold,
            'selected_features': selected_features.tolist(),
            'n_features': len(selected_features)
        })
        
        if len(selected_features) == 0:
            print(f"  ‚ö†Ô∏è  No features selected in fold {outer_fold + 1}")
            outer_scores.append(0.5)  # Random performance
            continue
        
        # Extract selected features
        X_train_selected = X_train_outer[:, selected_features]
        X_test_selected = X_test_outer[:, selected_features]
        
        # ‚úÖ Preprocessing: Fit ONLY on training fold
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_selected)
        X_test_scaled = scaler.transform(X_test_selected)  # Apply, don't fit!
        
        # ‚úÖ Model training: ONLY on training fold
        model = LogisticRegression(
            class_weight='balanced',
            max_iter=1000,
            random_state=42
        )
        model.fit(X_train_scaled, y_train_outer)
        
        # ‚úÖ Evaluation: ONLY on test fold
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        if len(set(y_test_outer)) >= 2:  # Both classes present
            auroc = roc_auc_score(y_test_outer, y_pred_proba)
            outer_scores.append(auroc)
            print(f"  ‚úÖ Fold {outer_fold + 1} AUROC: {auroc:.3f} ({len(selected_features)} features)")
        else:
            print(f"  ‚ö†Ô∏è  Fold {outer_fold + 1}: Only one class in test set")
            outer_scores.append(0.5)
    
    # Report results
    mean_auroc = np.mean(outer_scores)
    std_auroc = np.std(outer_scores)
    
    results = {
        'nested_cv_auroc': float(mean_auroc),
        'nested_cv_std': float(std_auroc),
        'fold_scores': [float(s) for s in outer_scores],
        'feature_selection_log': feature_selection_log
    }
    
    return results
```

**Common Mistakes to Avoid**:
1. ‚ùå **Selecting features before CV**: `features = select_features(X_full, y_full)` ‚Üí WRONG
2. ‚ùå **Fitting scaler on full data**: `scaler.fit(X_full)` ‚Üí WRONG
3. ‚ùå **Using test labels for selection**: `select_features(X_train, y_test)` ‚Üí WRONG
4. ‚ùå **Computing statistics on full data**: `mean = X_full.mean()` ‚Üí WRONG
5. ‚ùå **Hyperparameter tuning on test**: `best_C = tune_on_test(X_test, y_test)` ‚Üí WRONG

**Success Criteria**:
- ‚úÖ AUROC ‚â• 0.70 (maintains strong performance)
- ‚úÖ Variance ¬±0.10 or less (stable)
- ‚úÖ No data leakage (features selected within inner CV only)
- ‚úÖ Feature selection log shows different features per fold (if unstable)
- ‚úÖ All isolation checks pass (no train/test overlap)

**Output**: `ov_nested_cv_validation.json`

---

#### **Deliverable 1.2: Verify Aggregation Method** ‚úÖ **DONE**
**Timeline**: 1 hour  
**Status**: Already verified (mean() aggregation used)

**Task**: Confirm which aggregation method was used for 0.783
- **Result**: Mean() aggregation (matches manuscript)
- **Reference**: `.cursor/MOAT/SAE_INTELLIGENCE/BRCA_VALIDATION/DDR_BIN_AGGREGATION_VERIFICATION.md`

---

#### **Deliverable 1.3: Feature Stability Analysis**
**Timeline**: 1 day  
**Priority**: üî• **HIGH**

**Task**: Check if same 9 diamond features are selected across CV folds
- **Method**: Track which features are selected in each inner CV fold
- **Metric**: Feature selection frequency (how often each feature is selected)
- **Expected**: Top 9 diamonds should be selected in ‚â•80% of folds

**Script**: `scripts/sae/analyze_ov_feature_stability.py`

**Success Criteria**:
- ‚úÖ Top 9 diamonds selected in ‚â•80% of folds
- ‚úÖ Feature ranking consistent across folds
- ‚úÖ No high-variance features (selected in some folds, not others)

**Output**: `ov_feature_stability_analysis.json`

---

### **PHASE 2: MANUSCRIPT PREPARATION (Week 2)**

#### **Deliverable 2.1: Update Manuscript with Nested CV**
**Timeline**: 2 days  
**Priority**: üî• **HIGH**

**Task**: Update manuscript draft with nested CV results
- **Location**: `.cursor/MOAT/SAE_INTELLIGENCE/Publication-1/SAE_RESISTANCE/manuscript/MANUSCRIPT_DRAFT.md`
- **Changes**:
  - Update Results section with nested CV AUROC
  - Add Methods section describing nested CV procedure
  - Update Abstract if AUROC changes
  - Add limitations section (single cohort, no external validation)

**Success Criteria**:
- ‚úÖ Methods section clearly describes nested CV
- ‚úÖ Results match nested CV output
- ‚úÖ Abstract updated if needed
- ‚úÖ Limitations acknowledged

---

#### **Deliverable 2.2: Generate Publication Figures**
**Timeline**: 2 days  
**Priority**: üî• **HIGH**

**Task**: Create publication-quality figures

**Figure 1: ROC Curves**
- TRUE SAE vs PROXY SAE
- 5-fold CV curves (mean ¬± 95% CI)
- Include nested CV if different

**Figure 2: Pathway Mapping**
- 9 diamond features ‚Üí DDR pathway genes
- Heatmap of feature activation by variant
- Gene enrichment analysis

**Figure 3: DDR_bin Distribution**
- Resistant vs sensitive patients
- Box plots with statistical tests
- Optimal threshold (Youden's J)

**Figure 4: Feature Importance**
- Top 9 diamond features ranked by AUROC
- Individual feature performance
- Sparsity analysis (nonzero rate)

**Script**: `scripts/sae/generate_ov_publication_figures.py`

**Success Criteria**:
- ‚úÖ 300 DPI resolution
- ‚úÖ Publication-quality formatting
- ‚úÖ All statistical tests labeled
- ‚úÖ Consistent color scheme

**Output**: `figures/ov_roc_curves.png`, `figures/ov_pathway_map.png`, etc.

---

#### **Deliverable 2.3: Supplementary Materials**
**Timeline**: 1 day  
**Priority**: ‚ö†Ô∏è **MEDIUM**

**Task**: Create supplementary materials

**Content**:
1. **Feature List**: All 29 features (9 diamonds + 20 top)
2. **CV Results**: Per-fold AUROC, confusion matrices
3. **Pathway Enrichment**: Full gene enrichment results
4. **Code Availability**: GitHub link, reproducibility instructions
5. **Extended Methods**: Detailed SAE extraction procedure

**Script**: `scripts/sae/generate_ov_supplementary.py`

**Output**: `supplementary/ov_feature_list.csv`, `supplementary/ov_cv_results.json`, etc.

---

### **PHASE 3: EXTERNAL VALIDATION (Week 3-4)**

#### **Deliverable 3.1: Identify External Cohorts**
**Timeline**: 2 days  
**Priority**: ‚ö†Ô∏è **MEDIUM** (Nice to have, not required)

**Task**: Find independent ovarian cancer cohorts with:
- Platinum response labels
- WES/WGS data available
- ‚â•50 patients (ideally ‚â•100)

**Potential Cohorts**:
- **ICGC**: International Cancer Genome Consortium
- **GEO**: Gene Expression Omnibus (if expression available)
- **dbGaP**: Database of Genotypes and Phenotypes
- **Clinical Trials**: NCT datasets (if available)

**Success Criteria**:
- ‚úÖ At least 1 cohort identified
- ‚úÖ Data access confirmed (or application submitted)
- ‚úÖ Cohort characteristics documented

**Output**: `external_cohorts/ov_cohort_list.md`

---

#### **Deliverable 3.2: External Validation (If Available)**
**Timeline**: 1 week  
**Priority**: ‚ö†Ô∏è **LOW** (Optional, depends on data availability)

**Task**: Apply SAE model to external cohort
- **Method**: Use same 9 diamond features (no retraining)
- **Evaluation**: AUROC on external cohort
- **Expected**: AUROC 0.65-0.75 (some drop expected)

**Script**: `scripts/sae/validate_ov_external_cohort.py`

**Success Criteria**:
- ‚úÖ External AUROC ‚â• 0.65 (maintains reasonable performance)
- ‚úÖ Results reported in manuscript
- ‚úÖ Limitations discussed if performance drops

**Output**: `external_cohorts/ov_external_validation.json`

---

### **PHASE 4: MANUSCRIPT FINALIZATION (Week 4-5)**

#### **Deliverable 4.1: Final Manuscript Review**
**Timeline**: 2 days  
**Priority**: üî• **HIGH**

**Task**: Complete manuscript review
- **Sections**: Abstract, Introduction, Methods, Results, Discussion, References
- **Check**: All claims supported by data
- **Verify**: Statistical tests correct
- **Format**: Journal-specific formatting

**Success Criteria**:
- ‚úÖ All sections complete
- ‚úÖ Claims match results
- ‚úÖ Statistical tests verified
- ‚úÖ References complete

---

#### **Deliverable 4.2: Pre-submission Checklist**
**Timeline**: 1 day  
**Priority**: üî• **HIGH**

**Task**: Complete pre-submission checklist

**Checklist**:
- [ ] Nested CV validation complete
- [ ] All figures generated (300 DPI)
- [ ] Supplementary materials complete
- [ ] Code availability statement
- [ ] Data availability statement
- [ ] Author contributions
- [ ] Conflict of interest
- [ ] Funding statement
- [ ] Ethics approval (if needed)
- [ ] Statistical review complete

**Output**: `pre_submission_checklist.md`

---

## üìä SUCCESS METRICS

### **Validation Metrics**:
- ‚úÖ Nested CV AUROC ‚â• 0.70 (maintains strong performance)
- ‚úÖ Variance ¬±0.10 or less (stable)
- ‚úÖ Feature stability ‚â•80% (consistent feature selection)

### **Publication Metrics**:
- ‚úÖ Manuscript complete (all sections)
- ‚úÖ Figures publication-ready (300 DPI)
- ‚úÖ Supplementary materials complete
- ‚úÖ Pre-submission checklist complete

### **Timeline**:
- **Week 1**: Validation enhancement (Deliverables 1.1-1.3)
- **Week 2**: Manuscript preparation (Deliverables 2.1-2.3)
- **Week 3-4**: External validation (Deliverables 3.1-3.2) - Optional
- **Week 4-5**: Manuscript finalization (Deliverables 4.1-4.2)

**Total Timeline**: 4-5 weeks (3-4 weeks if skipping external validation)

---

## üéØ KEY DECISIONS

### **Decision 1: Nested CV vs Standard CV**
**Status**: ‚úÖ **DECIDED** - Use nested CV (prevents data leakage)

**Rationale**:
- BRCA validation revealed data leakage issue
- Nested CV is gold standard for feature selection
- Slight performance drop acceptable for rigor

### **Decision 2: External Validation**
**Status**: ‚ö†Ô∏è **OPTIONAL** - Proceed if data available, don't block on it

**Rationale**:
- External validation strengthens manuscript
- But not required for publication (TCGA is standard)
- Can add in revision if reviewers request

### **Decision 3: Feature Set**
**Status**: ‚úÖ **DECIDED** - Use 29 features (9 diamonds + 20 top)

**Rationale**:
- Already validated (AUROC 0.783)
- 9 diamonds provide biological interpretability
- 20 top features add predictive power

---

## üö® DATA LEAKAGE PREVENTION: COMPREHENSIVE SAFEGUARDS

### **Section 1: Code-Level Safeguards**

#### **1.1 Isolation Assertions**
```python
# REQUIRED in every validation script:
assert len(set(train_idx) & set(test_idx)) == 0, "TRAIN/TEST OVERLAP!"
assert len(set(train_ids) & set(test_ids)) == 0, "PATIENT ID OVERLAP!"
assert X_test.shape[0] == len(test_ids), "MISMATCH: X_test vs test_ids"
```

#### **1.2 Feature Selection Isolation**
```python
# ‚úÖ CORRECT: Select features on training only
for outer_fold in outer_cv:
    X_train, X_test, y_train, y_test = split_data()
    selected_features = select_features(X_train, y_train)  # ‚úÖ Training only
    model.fit(X_train[:, selected_features], y_train)
    score = evaluate(model, X_test[:, selected_features], y_test)

# ‚ùå WRONG: Select features on full data
selected_features = select_features(X_full, y_full)  # ‚ùå LEAKAGE!
for fold in cv:
    model.fit(X_train[:, selected_features], y_train)
    score = evaluate(model, X_test[:, selected_features], y_test)
```

#### **1.3 Preprocessing Isolation**
```python
# ‚úÖ CORRECT: Fit scaler on training, apply to test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # ‚úÖ Fit on train
X_test_scaled = scaler.transform(X_test)  # ‚úÖ Apply to test

# ‚ùå WRONG: Fit scaler on full data
scaler = StandardScaler()
X_full_scaled = scaler.fit_transform(X_full)  # ‚ùå LEAKAGE!
X_train, X_test = split(X_full_scaled)
```

#### **1.4 Statistical Test Isolation**
```python
# ‚úÖ CORRECT: Compute p-values on training only
for feature in features:
    train_resistant = X_train[y_train == 1, feature]
    train_sensitive = X_train[y_train == 0, feature]
    p_value = mannwhitneyu(train_resistant, train_sensitive).pvalue  # ‚úÖ Training only

# ‚ùå WRONG: Compute p-values on full data
for feature in features:
    full_resistant = X_full[y_full == 1, feature]
    full_sensitive = X_full[y_full == 0, feature]
    p_value = mannwhitneyu(full_resistant, full_sensitive).pvalue  # ‚ùå LEAKAGE!
```

---

### **Section 2: Validation Checks**

#### **2.1 Pre-Validation Checks** (Before Running CV)
- [ ] **Data Integrity**: No duplicate patient IDs
- [ ] **Label Integrity**: All patients have valid labels (no NaN)
- [ ] **Feature Integrity**: All features have valid values (no NaN in critical features)
- [ ] **Class Balance**: Both classes present (resistant + sensitive)
- [ ] **Sample Size**: Sufficient patients per class (‚â•10 per class recommended)

#### **2.2 During-Validation Checks** (Inside CV Loop)
- [ ] **Fold Isolation**: No patient appears in both train and test
- [ ] **Feature Selection**: Features selected only on training fold
- [ ] **Preprocessing**: Scaler/normalizer fit only on training fold
- [ ] **Model Training**: Model trained only on training fold
- [ ] **Evaluation**: Test fold used only for final evaluation

#### **2.3 Post-Validation Checks** (After CV)
- [ ] **Score Distribution**: CV scores are reasonable (not all 1.0 or 0.5)
- [ ] **Variance Check**: CV std < 0.15 (high variance = instability)
- [ ] **Feature Stability**: Same features selected across folds (‚â•80% overlap)
- [ ] **Performance Drop**: Nested CV AUROC within 5 pp of original (if >5 pp drop, investigate)

---

### **Section 3: Common Mistakes & How to Avoid**

#### **Mistake 1: Feature Selection Before CV**
```python
# ‚ùå WRONG:
selected_features = select_features(X_full, y_full)  # Leakage!
for fold in cv:
    model.fit(X_train[:, selected_features], y_train)

# ‚úÖ CORRECT:
for fold in cv:
    selected_features = select_features(X_train, y_train)  # Per fold
    model.fit(X_train[:, selected_features], y_train)
```

#### **Mistake 2: Computing Statistics on Full Data**
```python
# ‚ùå WRONG:
mean = X_full.mean(axis=0)  # Leakage!
std = X_full.std(axis=0)
X_train_norm = (X_train - mean) / std

# ‚úÖ CORRECT:
mean = X_train.mean(axis=0)  # Training only
std = X_train.std(axis=0)
X_train_norm = (X_train - mean) / std
X_test_norm = (X_test - mean) / std  # Apply same transformation
```

#### **Mistake 3: Using Test Labels for Selection**
```python
# ‚ùå WRONG:
selected_features = select_features(X_train, y_test)  # Using test labels!

# ‚úÖ CORRECT:
selected_features = select_features(X_train, y_train)  # Training labels only
```

#### **Mistake 4: Hyperparameter Tuning on Test**
```python
# ‚ùå WRONG:
best_C = tune_hyperparameters(X_test, y_test)  # Tuning on test!

# ‚úÖ CORRECT:
# Use inner CV for hyperparameter tuning
for C in [0.1, 1.0, 10.0]:
    inner_scores = []
    for inner_fold in inner_cv:
        model = LogisticRegression(C=C)
        model.fit(X_train_inner, y_train_inner)
        score = evaluate(model, X_val_inner, y_val_inner)
        inner_scores.append(score)
    if np.mean(inner_scores) > best_score:
        best_C = C
```

#### **Mistake 5: Aggregating Features Across Folds**
```python
# ‚ùå WRONG:
all_selected_features = []
for fold in cv:
    selected = select_features(X_train, y_train)
    all_selected_features.extend(selected)
final_features = set(all_selected_features)  # Using all features from all folds

# ‚úÖ CORRECT:
# Select features independently per fold, or use consensus
feature_counts = {}
for fold in cv:
    selected = select_features(X_train, y_train)
    for feat in selected:
        feature_counts[feat] = feature_counts.get(feat, 0) + 1
consensus_features = [f for f, count in feature_counts.items() if count >= 3]  # Selected in ‚â•3 folds
```

---

### **Section 4: Verification Protocol**

#### **4.1 Code Review Checklist**
Before running validation, verify:
- [ ] No `X_full` or `y_full` used in feature selection
- [ ] All scalers/normalizers use `.fit()` only on training
- [ ] All statistical tests use training data only
- [ ] All model training uses training data only
- [ ] Test fold used only for final evaluation
- [ ] Assertions added to catch overlaps

#### **4.2 Results Review Checklist**
After validation, verify:
- [ ] CV scores are reasonable (0.5-1.0 range)
- [ ] CV variance is acceptable (<0.15)
- [ ] Feature selection is stable (‚â•80% overlap across folds)
- [ ] Performance drop is reasonable (<5 pp from original)
- [ ] No suspicious patterns (all folds = 1.0 or 0.5)

#### **4.3 Comparison to Original**
Compare nested CV to original 0.783:
- **If nested CV ‚âà 0.783**: ‚úÖ No leakage in original (good!)
- **If nested CV < 0.75**: ‚ö†Ô∏è Possible leakage in original (investigate)
- **If nested CV < 0.70**: üö® Significant leakage (original invalid)

---

## üö® RISKS & MITIGATION

### **Risk 1: Data Leakage (CRITICAL)**
**Probability**: High (if not careful)  
**Impact**: Critical (invalidates results)

**Mitigation**:
- ‚úÖ Use nested CV (mandatory)
- ‚úÖ Add isolation assertions (code-level)
- ‚úÖ Code review checklist (pre-validation)
- ‚úÖ Results review checklist (post-validation)
- ‚úÖ Compare to original (detect leakage)

### **Risk 2: Nested CV Performance Drop**
**Probability**: Medium  
**Impact**: High

**Mitigation**:
- Accept AUROC ‚â• 0.70 as success
- Document performance drop in limitations
- Emphasize biological validation (pathway mapping)
- If drop >5 pp, investigate original validation

### **Risk 3: Feature Instability**
**Probability**: Low  
**Impact**: Medium

**Mitigation**:
- Check feature stability (Deliverable 1.3)
- Use consensus features (selected in ‚â•80% folds)
- Document feature selection procedure
- Report feature selection frequency

### **Risk 4: External Cohort Unavailable**
**Probability**: High  
**Impact**: Low

**Mitigation**:
- External validation is optional
- TCGA is standard benchmark
- Can add in revision if requested

---

## üìù NEXT STEPS

### **Immediate (This Week)**:
1. ‚úÖ **Start Deliverable 1.1**: Nested CV validation (2 days)
   - **CRITICAL**: Implement all data leakage safeguards
   - **CRITICAL**: Add isolation assertions
   - **CRITICAL**: Verify no train/test overlap
2. ‚úÖ **Start Deliverable 1.3**: Feature stability analysis (1 day)
   - Track feature selection per fold
   - Compute feature selection frequency
   - Identify consensus features
3. ‚úÖ **Code Review**: Review validation script for leakage (1 day)
   - Check all isolation assertions
   - Verify preprocessing isolation
   - Verify feature selection isolation
4. ‚úÖ **Review existing manuscript**: Identify gaps (1 day)

### **Next Week**:
5. ‚ö†Ô∏è **Deliverable 2.1**: Update manuscript with nested CV
6. ‚ö†Ô∏è **Deliverable 2.2**: Generate publication figures

---

## ‚úÖ VALIDATION PROTOCOL SUMMARY

### **Before Running Validation**:
1. ‚úÖ Review code for data leakage risks
2. ‚úÖ Add isolation assertions
3. ‚úÖ Verify data integrity (no duplicates, no NaN)
4. ‚úÖ Check class balance

### **During Validation**:
1. ‚úÖ Run nested CV with isolation checks
2. ‚úÖ Log feature selection per fold
3. ‚úÖ Verify no train/test overlap
4. ‚úÖ Monitor CV scores for suspicious patterns

### **After Validation**:
1. ‚úÖ Compare nested CV to original (detect leakage)
2. ‚úÖ Check feature stability (‚â•80% overlap)
3. ‚úÖ Verify CV variance (<0.15)
4. ‚úÖ Document any performance drop

---

## ‚úÖ VALIDATION SUMMARY

**Current Status**:
- ‚úÖ AUROC 0.783 ¬± 0.100 (5-fold CV, 149 patients)
- ‚úÖ +15.5 pp improvement over baseline
- ‚úÖ Biological validation (DDR pathway mapping)
- ‚ö†Ô∏è Needs nested CV (prevent leakage)

**Target Status**:
- ‚úÖ Nested CV AUROC ‚â• 0.70
- ‚úÖ Publication-ready manuscript
- ‚úÖ Publication-quality figures
- ‚úÖ Ready for submission

---

**Status**: üöÄ **READY TO EXECUTE** - Start with Deliverable 1.1 (Nested CV)  
**Timeline**: 4-5 weeks to submission-ready manuscript  
**Priority**: üî• **HIGHEST** - Ovarian is publication-ready, BRCA is not

---

## üìã QUICK REFERENCE: DATA LEAKAGE PREVENTION

### **üö® THE GOLDEN RULE**
> **NEVER use test data (or test labels) for ANY step of model development.**
> 
> Feature selection, preprocessing, hyperparameter tuning, and statistical tests must ALL use training data only.

### **‚úÖ DO THIS**:
```python
# Feature selection: Training only
selected = select_features(X_train, y_train)

# Preprocessing: Fit on training, apply to test
scaler.fit(X_train)
X_test_scaled = scaler.transform(X_test)

# Statistical tests: Training only
p_value = test_statistic(X_train[y_train==1], X_train[y_train==0])

# Model training: Training only
model.fit(X_train, y_train)

# Evaluation: Test only
score = evaluate(model, X_test, y_test)
```

### **‚ùå NEVER DO THIS**:
```python
# ‚ùå Feature selection on full data
selected = select_features(X_full, y_full)

# ‚ùå Preprocessing on full data
scaler.fit(X_full)

# ‚ùå Statistical tests on full data
p_value = test_statistic(X_full[y_full==1], X_full[y_full==0])

# ‚ùå Using test labels for selection
selected = select_features(X_train, y_test)

# ‚ùå Training on test data
model.fit(X_test, y_test)
```

### **üîç RED FLAGS** (Indicators of Leakage):
- üö® CV AUROC > 0.95 (suspiciously high)
- üö® All CV folds have identical scores
- üö® Nested CV drops >10 pp from original
- üö® Feature selection identical across all folds (if unstable)
- üö® Model performance on test = training performance

### **‚úÖ VERIFICATION STEPS**:
1. **Before CV**: Add isolation assertions
2. **During CV**: Log feature selection per fold
3. **After CV**: Compare nested CV to original
4. **Code Review**: Check all preprocessing/scaling
5. **Results Review**: Check for suspicious patterns

---

## üéØ KEY TAKEAWAYS

1. **Nested CV is MANDATORY** - Not optional for feature selection
2. **Isolation is CRITICAL** - Train/test must never overlap
3. **Preprocessing must be FIT on training only** - Never on full data
4. **Feature selection must be PER FOLD** - Not before CV
5. **Verification is ESSENTIAL** - Check results for leakage signs

**Remember**: The BRCA validation caught 23.7 pp of leakage. Don't repeat that mistake!
