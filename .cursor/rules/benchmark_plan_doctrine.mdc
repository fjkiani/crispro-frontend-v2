---
alwaysApply: false
description: Benchmark & Evidence Plan – What we will prove, datasets, scaffolding, scenarios, and metrics
---
# Benchmark & Evidence Plan – From Signals to Trustworthy Guidance

This rule defines what we are trying to prove, which datasets we will use, the experimental scaffolding, success metrics, and how these results map to clinical guidance (Tier I/II/III/Research). It is the execution playbook for turning our S/P/E + insights pipeline into measurable, trustworthy, and auditable performance.

## 1) What we will prove (hypotheses)

- H1 – Variant impact accuracy (coding/noncoding proxy):
  Evo2 `min_delta`/`exon_delta` can discriminate ClinVar pathogenic vs benign SNVs with strong AUROC/AUPRC.
- H2 – DDR/HRD → platinum/PARP sensitivity:
  Our synthetic lethality path (damage + dependency) identifies platinum/PARP responders with meaningful AUPRC on public cohorts.
- H3 – Tier integrity and calibration:
  Tier I (Yes GO) correlates with on‑label/guideline truth; model‑only Tier I operating points can be chosen to achieve target precision/FP rates.

## 2) Primary metrics and analyses

- Variant impact (ClinVar): AUROC, AUPRC; reliability (ECE/Brier) for calibrated scores.
- HRD/platinum cohorts: AUPRC (preferred), AUROC; threshold analysis for operating points.
- Guidance evaluation: Precision/Recall/F1 of Tier I vs guideline truth; ablations (±Chromatin, ±Regulatory, ±Essentiality lifts).

## 3) Datasets (minimum viable set)

- ClinVar variant_summary (tab‑delimited): SNVs only, GRCh38 preferred; filter conflicts; label positive = pathogenic/likely_pathogenic, negative = benign/likely_benign.
  - Source: NCBI FTP (`variant_summary.txt.gz`).
  - Parser/bench: `[variant_auroc.py](mdc:tools/benchmarks/variant_auroc.py)`.
- DDR/HRD platinum response (public): small ovarian/breast cohorts with HRD labels and platinum outcomes (to be enumerated; allow substitution with internal if available).
- Guidance truth: FDA/DailyMed on‑label status + open guidelines (NCCN/ESMO summaries where license permits) for disease/therapy pairs.

## 4) Experimental scaffolding (code + scripts)

- Variant AUROC/AUPRC (ready):
  - Script: `[variant_auroc.py](mdc:tools/benchmarks/variant_auroc.py)`
  - Runs Evo proxy endpoints: `[evo.py](mdc:oncology-coPilot/oncology-backend-minimal/api/routers/evo.py)`
  - Output JSON: AUROC/AUPRC and configs.
  - Extend: flags to include `likely_*`, GRCh37, and `exon_delta` aggregation.
- HRD/platinum AUPRC (plan):
  - Script: `tools/benchmarks/hrd_platinum_auprc.py` (to be created)
  - Flow: compute damage (VEP proxy + functionality), dependency (essentiality), map to platinum/PARP, compare to outcomes.
- Guidance tier eval (plan):
  - Script: `tools/benchmarks/guidance_tier_eval.py` (to be created)
  - Flow: compare Tier I vs on‑label/guideline truth; compute precision/recall/F1.
- Makefile targets (ready):
  - Backend: `make backend`
  - Chromatin proxies: `make chromatin`
  - Add: `make bench_variant` (run AUROC script with defaults).

## 5) Endpoints under test (where signals originate)

- Orchestrator: `[efficacy.py](mdc:oncology-coPilot/oncology-backend-minimal/api/routers/efficacy.py)`
- Insights: `[insights.py](mdc:oncology-coPilot/oncology-backend-minimal/api/routers/insights.py)`
  - `predict_gene_essentiality`, `predict_chromatin_accessibility` (Enformer/Borzoi), `predict_protein_functionality_change`, `predict_splicing_regulatory`
- Guidance: `[guidance.py](mdc:oncology-coPilot/oncology-backend-minimal/api/routers/guidance.py)`
  - `chemo`, `radonc`, `synthetic_lethality`
- Evo proxy: `[evo.py](mdc:oncology-coPilot/oncology-backend-minimal/api/routers/evo.py)`

## 6) Test scenarios (minimum set)

- Variant AUROC:
  - Balanced SNVs: 1,000 pathogenic/likely_pathogenic vs 1,000 benign/likely_benign; report AUROC/AUPRC.
  - Sensitivity: coding vs noncoding subsets; `min_delta` vs `exon_delta` vs combined.
- DDR/HRD AUPRC:
  - Ovarian cohort: BRCA1/2 LOF vs platinum outcomes; report AUPRC; ablate essentiality and chromatin lifts.
  - Breast cohort: replicate method; compare operating points.
- Guidance tiering:
  - Chemo: disease/therapy pairs with known label; compute Tier I precision/recall; measure effect of guideline/RCT/meta badges.
  - RadOnc: NSCLC TP53 scenarios; verify stability and tier transitions with evidence gates.
  - Off‑label “model‑only” gate: test threshold grid for efficacy/confidence/evidence_strength → select operating point from ROC/PR.

## 7) Mapping metrics → product gates

- Calibrate internal signals (Platt/Isotonic) on held‑out; select probability cutoffs to match target PR operating points.
- Encode thresholds in guidance:
  - Experimental Tier I requires: `efficacy_score ≥ T_efficacy`, `confidence ≥ T_conf`, `evidence_strength ≥ T_evidence`, MoA alignment true, minimal literature signal present.
  - Tier I (label) unchanged: on‑label/guideline present.
- UI clarity: badge “Model‑backed Yes” vs “Label‑backed Yes”; show expected precision from validation curve.

## 8) Governance & auditability

- Persist benchmark outputs (JSON) with run signatures and environment (model_id, API base, dates).
- Record lifts (Functionality/Chromatin/Essentiality/Regulatory) and their provenance in results.
- Maintain reproducible Makefile targets and pin environment in `requirements.txt` if dedicated venv is used.

## 9) Timeline & milestones

- Week 1: Ship variant AUROC/AUPRC (balanced SNVs), add flags, publish JSON + short write‑up.
- Week 2: Implement guidance tier eval; fetch on‑label/guideline truth (ruleset stub then FDA/DailyMed).
- Week 3–4: Implement HRD/platinum AUPRC on at least one open cohort; ablation study; select model‑only Tier I thresholds.

## 10) Execution checklist (quick)

- [ ] ClinVar AUROC run ≥1k/1k with JSON results
- [ ] Add `make bench_variant` target with sane defaults
- [ ] Implement `guidance_tier_eval.py` with label truth CSV
- [ ] Implement `hrd_platinum_auprc.py` and cohort loader
- [ ] Document thresholds → router gates; update UI badges and copy

