# AYESHA Benchmark Improvement Plan

**Date**: January 27, 2025  
**Based on**: Review of SOTA benchmark scripts structure  
**Purpose**: Improve AYESHA benchmark scripts to follow best practices from SOTA benchmarks

---

## üìä Review of SOTA Benchmark Structure

### What SOTA Benchmarks Do Well

#### 1. **Clean, Simple Structure**
- **Pattern**: Simple async functions, not class-based
- **Example**: `async def run_mm_benchmark()` - single entry point
- **Benefit**: Easier to read, maintain, and debug

#### 2. **Standardized Output Format**
```python
benchmark_result = {
    "timestamp": datetime.now().isoformat(),
    "disease": "multiple_myeloma",
    "metrics": {
        "pathway_alignment_accuracy": accuracy,
        "correct_predictions": correct_predictions,
        "total_predictions": total_predictions,
        "average_confidence": avg_confidence,
    },
    "target": {
        "pathway_alignment_accuracy": 1.0,  # Clear target
    },
    "results": results,  # Individual test results
    "provenance": {
        "script": "benchmark_sota_mm.py",
        "api_base": API_ROOT,
        "model_id": "evo2_1b",
        "ablation_mode": "SPE",
    }
}
```
- **Benefit**: Consistent format across all benchmarks, easy to parse/compare

#### 3. **Clear Metrics & Targets**
- **MM**: Single metric (pathway_alignment_accuracy), target = 1.0 (100%)
- **Ovarian**: Single metric (AUROC), target = 0.75 (stretch), 0.65 (minimum)
- **Melanoma**: Single metric (drug_ranking_accuracy), target = 0.90 (90%)
- **Benefit**: Clear success criteria, easy to interpret

#### 4. **Consistent Output Directory**
- **Pattern**: `results/sota/{disease}_benchmark_{timestamp}.json`
- **Example**: `results/sota/mm_benchmark_20250127_143022.json`
- **Benefit**: Easy to find and compare results

#### 5. **Clear Console Output**
```python
print("üß¨ Running MM SOTA Benchmark...")
print(f"  Testing {variant['gene']} {variant['hgvs_p']}...")
print(f"    {status} Expected: {expected_drug}, Got: {result.get('top_drug')}")
print(f"\n‚úÖ Benchmark complete!")
print(f"   Accuracy: {accuracy:.1%} ({correct_predictions}/{total_predictions})")
```
- **Benefit**: Easy to see progress and results in real-time

#### 6. **Simple Error Handling**
```python
if "error" in result:
    print(f"    ‚úó Error: {result['error']}")
    continue  # Skip and continue
```
- **Benefit**: Graceful degradation, doesn't crash on single failure

#### 7. **Focused Test Data**
- **MM**: 7 variants (5 MAPK, 2 TP53)
- **Ovarian**: TCGA-OV dataset (1k variants)
- **Melanoma**: 2 known driver mutations
- **Benefit**: Clear, focused test cases

---

## üîç Current AYESHA Benchmark Structure

### What AYESHA Benchmarks Have

#### 1. **Class-Based Structure**
- **Pattern**: `BenchmarkRunner` class with test methods
- **Example**: `async def test_pathway_accuracy(self)`
- **Complexity**: More complex, but allows shared state

#### 2. **Multiple Test Dimensions**
- **5 Test Types**: Pathway, Drug, Mechanism, Synthetic Lethality, Evidence
- **15+ Individual Tests**: Each dimension has multiple sub-tests
- **Benefit**: Comprehensive validation, but harder to summarize

#### 3. **Detailed Ground Truth**
- **Comprehensive**: Variants, drugs, pathways, mechanism vectors, synthetic lethality
- **Benefit**: Very detailed, but verbose

#### 4. **Detailed Output Format**
- **Per-Test Results**: Each test has pass/fail, score, expected, actual
- **Overall Summary**: Total tests, passed, failed, average score
- **Benefit**: Very detailed, but harder to parse programmatically

#### 5. **Output Directory**
- **Pattern**: `results/benchmarks/ayesha_accuracy_benchmark_{timestamp}.json`
- **Benefit**: Consistent with other benchmarks

---

## üí° Improvement Plan

### Phase 1: Standardize Output Format (High Priority)

**Goal**: Make AYESHA benchmark output format consistent with SOTA benchmarks

#### Changes Needed:

1. **Add Standardized Structure**
   ```python
   benchmark_result = {
       "timestamp": datetime.now().isoformat(),
       "benchmark_type": "ayesha_consistency",  # vs "sota_performance"
       "case": "MBD4+TP53_HGSOC",
       "metrics": {
           "overall_pass_rate": pass_rate,
           "pathway_accuracy": pathway_accuracy,
           "drug_accuracy": drug_accuracy,
           "mechanism_accuracy": mechanism_accuracy,
           "synthetic_lethality_accuracy": synthetic_lethality_accuracy,
           "evidence_alignment_accuracy": evidence_alignment_accuracy,
       },
       "targets": {
           "overall_pass_rate": 0.85,  # 85%
           "pathway_accuracy": 0.80,
           "drug_accuracy": 0.90,  # Critical for safety
           "mechanism_accuracy": 0.80,
       },
       "results": {
           "pathway_tests": [...],
           "drug_tests": [...],
           "mechanism_tests": [...],
           "synthetic_lethality_tests": [...],
           "evidence_tests": [...],
       },
       "provenance": {
           "script": "benchmark_mbd4_tp53_accuracy.py",
           "api_base": API_ROOT,
           "model_id": "evo2_1b",
           "ground_truth_source": "biology_assumptions + nccn_guidelines",
           "validation_type": "consistency_alignment",  # Not "real_accuracy"
       }
   }
   ```

2. **Add Clear Metrics Summary**
   - Compute aggregate metrics per dimension
   - Add pass/fail status for each dimension
   - Add overall pass/fail status

3. **Standardize Output Directory**
   - Keep: `results/benchmarks/` (consistent with current)
   - Format: `ayesha_consistency_benchmark_{timestamp}.json`
   - Add: `results/benchmarks/ayesha/` subdirectory for organization

**Files to Modify**:
- `scripts/benchmark_mbd4_tp53_accuracy.py` - Main benchmark script
- `scripts/benchmark_clinical_validation.py` - Clinical validation script
- `scripts/benchmark_brca_tp53_proxy.py` - BRCA proxy script

---

### Phase 2: Simplify Structure (Medium Priority)

**Goal**: Make AYESHA benchmarks easier to read and maintain

#### Option A: Keep Class-Based (Recommended for AYESHA)

**Rationale**: AYESHA has 5 test dimensions with 15+ tests. Class-based structure helps organize this complexity.

**Improvements**:
1. **Extract Helper Functions**: Move API calls to separate functions
2. **Simplify Test Methods**: Each test method should be <50 lines
3. **Add Clear Test Organization**: Group related tests together

#### Option B: Convert to Function-Based (Like SOTA)

**Rationale**: Simpler structure, easier to read

**Trade-off**: Would need to pass state between functions (more parameters)

**Recommendation**: **Keep class-based** for AYESHA (complexity justifies it), but extract helper functions.

---

### Phase 3: Improve Console Output (Medium Priority)

**Goal**: Make console output clearer and more informative

#### Changes Needed:

1. **Add Progress Indicators**
   ```python
   print("üß¨ Running AYESHA Consistency Benchmark...")
   print("  Testing MBD4+TP53 HGSOC case...")
   print("\n  üìä Pathway Accuracy Tests...")
   print("    ‚úÖ DDR pathway: 1.000 (expected: 0.9-1.0)")
   print("    ‚úÖ TP53 pathway: 0.800 (expected: 0.7-0.9)")
   print("\n  üíä Drug Recommendation Tests...")
   print("    ‚úÖ PARP inhibitors: Rank #1-3 (expected: Top 3)")
   print("    ‚úÖ Platinum: Rank #4 (expected: #4)")
   print("\n‚úÖ Benchmark complete!")
   print(f"   Overall Pass Rate: {pass_rate:.1%} ({passed}/{total})")
   print(f"   Pathway Accuracy: {pathway_accuracy:.1%}")
   print(f"   Drug Accuracy: {drug_accuracy:.1%}")
   print(f"   Results saved to: {output_file}")
   ```

2. **Add Summary Table**
   ```python
   print("\n" + "="*80)
   print("AYESHA BENCHMARK SUMMARY")
   print("="*80)
   print(f"{'Dimension':<30} {'Status':<10} {'Score':<10} {'Target':<10}")
   print("-"*80)
   print(f"{'Pathway Accuracy':<30} {'‚úÖ PASS':<10} {pathway_accuracy:.1%}     {'‚â•80%':<10}")
   print(f"{'Drug Accuracy':<30} {'‚úÖ PASS':<10} {drug_accuracy:.1%}     {'‚â•90%':<10}")
   print(f"{'Mechanism Accuracy':<30} {'‚úÖ PASS':<10} {mechanism_accuracy:.1%}     {'‚â•80%':<10}")
   print("="*80)
   ```

3. **Add Clear Error Messages**
   - Show which test failed
   - Show expected vs actual
   - Show error details (if any)

---

### Phase 4: Clinical Trial Outcome Benchmarking (NEW - High Priority)

**Goal**: Benchmark our system's ability to predict clinical trial outcomes (response rates, PFS, OS)

#### What We Can Benchmark:

1. **Response Rate Prediction**
   - **Input**: Published RCT response rates (e.g., SOLO-2: 65% ORR for olaparib in BRCA+TP53)
   - **Our Prediction**: Efficacy scores for same drug in similar patient population
   - **Metric**: Correlation between efficacy scores and published response rates
   - **Example**: Do patients with higher efficacy scores (0.8-0.9) correspond to trials with higher response rates (60-70%)?

2. **Progression-Free Survival (PFS) Prediction**
   - **Input**: Published median PFS (e.g., PRIMA: 13.8 months for niraparib in HRD+)
   - **Our Prediction**: Efficacy scores + confidence scores
   - **Metric**: Correlation between efficacy scores and median PFS
   - **Example**: Do higher efficacy scores correlate with longer PFS?

3. **Overall Survival (OS) Prediction**
   - **Input**: Published median OS (e.g., PAOLA-1: 37.2 months for olaparib+bevacizumab)
   - **Our Prediction**: Efficacy scores + mechanism vectors
   - **Metric**: Correlation between mechanism fit and OS
   - **Example**: Do patients with higher mechanism fit have better OS?

4. **Drug Ranking Accuracy**
   - **Input**: Published trial results showing which drugs worked best
   - **Our Prediction**: Drug rankings from efficacy scores
   - **Metric**: Top-3 accuracy (did we rank the best-performing drug in top 3?)
   - **Example**: If olaparib had highest response rate, is it ranked #1-3 by our system?

#### Data Sources:

**Two Approaches Available**:

##### Approach A: Published RCT Results (Aggregate Data)
1. **Published RCT Results** (Primary Source)
   - **SOLO-2**: Olaparib maintenance in BRCA+TP53 (65% ORR)
   - **PAOLA-1**: Olaparib+bevacizumab in HRD+ (64% ORR)
   - **PRIMA**: Niraparib in HRD+ (57% ORR)
   - **ARIEL3**: Rucaparib in BRCA+TP53 (64% ORR)
   - **NOVA**: Niraparib maintenance (PFS data)
   - **Source**: Published papers, ClinicalTrials.gov results
   - **Ground Truth**: Aggregate response rates (trial-level, not patient-level)

2. **Trial Patient Populations**
   - **Inclusion Criteria**: Extract from trial descriptions
   - **Biomarkers**: HRD+, BRCA+, TP53 status
   - **Disease Type**: Ovarian cancer, specific histology
   - **Line of Therapy**: First-line, maintenance, etc.

##### Approach B: cBioPortal Patient Data (Individual Patient Data) ‚≠ê NEW
1. **cBioPortal Studies** (via pyBioPortal library)
   - **Library**: `pybioportal` (already in repo: `oncology-coPilot/oncology-backend/tests/pyBioPortal-master`)
   - **Target Studies**: `ov_tcga_pan_can_atlas_2018`, `ov_tcga` (ovarian cancer cohorts)
   - **What We Extract**:
     - **Mutations**: Patient-level mutations (BRCA1, TP53, etc.)
     - **Clinical Outcomes**: Individual patient response rates, PFS, OS
     - **Treatments**: Patient-level treatment data (drugs, response)
   - **Ground Truth**: Individual patient outcomes (patient-level, not aggregate)
   - **Advantage**: Can validate individual patient predictions, not just aggregate correlations

2. **Our System Predictions**
   - **Endpoint**: `POST /api/efficacy/predict`
   - **Input**: Patient mutations (from cBioPortal), disease, tumor context
   - **Output**: Efficacy scores, confidence, evidence tiers, drug rankings
   - **For Each Patient**: Run prediction, compare with actual outcome

**Recommended: Hybrid Approach**
- **Use Both**: Published RCT results (aggregate validation) + cBioPortal data (individual patient validation)
- **Why**: RCT results validate against published trials (gold standard), cBioPortal validates individual predictions (more granular)

**See**: `.cursor/ayesha/PYBIOPORTAL_CLINICAL_TRIAL_EXTRACTION_PLAN.md` for detailed extraction strategy

#### Benchmark Structure:

```python
class ClinicalTrialOutcomeBenchmark:
    """Benchmark efficacy predictions against published trial outcomes."""
    
    def __init__(self):
        self.trial_database = self._load_trial_database()  # Published RCT results
        self.client = httpx.AsyncClient(base_url=API_ROOT)
    
    async def test_response_rate_correlation(self):
        """Test: Do efficacy scores correlate with published response rates?"""
        # For each trial:
        # 1. Extract patient population (mutations, biomarkers)
        # 2. Run our system: POST /api/efficacy/predict
        # 3. Get efficacy scores for trial drug
        # 4. Compare: efficacy_score vs. published_response_rate
        # 5. Compute correlation (Pearson, Spearman)
        pass
    
    async def test_pfs_correlation(self):
        """Test: Do efficacy scores correlate with median PFS?"""
        # Similar to response rate, but compare with PFS data
        pass
    
    async def test_drug_ranking_accuracy(self):
        """Test: Do we rank best-performing drugs in top 3?"""
        # For each trial:
        # 1. Identify best-performing drug (highest response rate)
        # 2. Get our drug rankings
        # 3. Check: Is best-performing drug in top 3?
        # 4. Compute top-3 accuracy
        pass
    
    async def test_mechanism_fit_vs_outcomes(self):
        """Test: Does mechanism fit correlate with trial outcomes?"""
        # For each trial:
        # 1. Get patient mechanism vector
        # 2. Get trial MoA vector
        # 3. Compute mechanism fit (cosine similarity)
        # 4. Compare: mechanism_fit vs. response_rate/PFS
        pass
```

#### Output Format:

```python
trial_outcome_benchmark = {
    "timestamp": datetime.now().isoformat(),
    "benchmark_type": "clinical_trial_outcomes",
    "metrics": {
        "response_rate_correlation": {
            "pearson_r": 0.75,
            "spearman_rho": 0.72,
            "p_value": 0.001,
            "n_trials": 12
        },
        "pfs_correlation": {
            "pearson_r": 0.68,
            "spearman_rho": 0.65,
            "p_value": 0.005,
            "n_trials": 8
        },
        "drug_ranking_accuracy": {
            "top_1_accuracy": 0.60,  # Best drug ranked #1
            "top_3_accuracy": 0.85,  # Best drug in top 3
            "n_trials": 12
        },
        "mechanism_fit_correlation": {
            "pearson_r": 0.55,
            "spearman_rho": 0.52,
            "p_value": 0.02,
            "n_trials": 12
        }
    },
    "targets": {
        "response_rate_correlation": {"min_pearson_r": 0.60, "min_spearman_rho": 0.55},
        "drug_ranking_accuracy": {"min_top_3": 0.75},
        "mechanism_fit_correlation": {"min_pearson_r": 0.50}
    },
    "results": [
        {
            "trial_id": "SOLO-2",
            "nct_id": "NCT01874353",
            "drug": "olaparib",
            "published_response_rate": 0.65,
            "our_efficacy_score": 0.82,
            "our_rank": 1,
            "published_rank": 1,  # Best in trial
            "mechanism_fit": 0.88,
            "correlation_status": "‚úÖ PASS"
        },
        # ... more trials
    ],
    "provenance": {
        "script": "benchmark_clinical_trial_outcomes.py",
        "trial_database": "published_rct_results.json",
        "api_base": API_ROOT,
        "model_id": "evo2_1b",
        "validation_type": "real_outcome_validation"
    }
}
```

#### Implementation Steps:

**Phase 4A: Extract cBioPortal Datasets** (2-3 days) ‚≠ê NEW - Using pyBioPortal

1. **Create Extraction Script** (`scripts/benchmark/extract_cbioportal_trial_datasets.py`)
   - Use `pybioportal` library to access cBioPortal API
   - Find target studies (ovarian cancer: `ov_tcga_pan_can_atlas_2018`)
   - Extract mutations, clinical outcomes, treatments
   - Combine into unified dataset
   - Save to `data/benchmarks/cbioportal_trial_datasets.json`
   - **See**: `.cursor/ayesha/PYBIOPORTAL_CLINICAL_TRIAL_EXTRACTION_PLAN.md` for details

2. **Create Dataset Validator** (`scripts/benchmark/validate_cbioportal_datasets.py`)
   - Check data completeness (mutations, outcomes, treatments)
   - Filter to patients with complete data
   - Report data quality metrics

3. **Test Extraction** (1 day)
   - Run on `ov_tcga_pan_can_atlas_2018`
   - Verify data structure
   - Check data completeness

**Phase 4B: Extract Published RCT Results** (2-3 days)

1. **Create Trial Database** (`scripts/data/published_rct_results.json`)
   - Extract published RCT results (response rates, PFS, OS)
   - Map to patient populations (mutations, biomarkers)
   - Structure as JSON database
   - **Source**: Published papers, ClinicalTrials.gov results

**Phase 4C: Create Benchmark Scripts** (2-3 days)

1. **cBioPortal Benchmark** (`scripts/benchmark/benchmark_clinical_trial_outcomes_cbioportal.py`)
   - Load cBioPortal datasets
   - For each patient: Run `/api/efficacy/predict`
   - Compare predictions vs. actual outcomes (individual patient level)
   - Compute correlation metrics (Pearson, Spearman)
   - **Advantage**: Patient-level validation, not just aggregate

2. **Published RCT Benchmark** (`scripts/benchmark/benchmark_clinical_trial_outcomes_rct.py`)
   - Load published RCT results
   - For each trial: Run our system on trial population
   - Compare predictions vs. published outcomes (aggregate level)
   - Compute correlation metrics
   - **Advantage**: Validates against published clinical trials (gold standard)

3. **Add Correlation Analysis** (1-2 days)
   - Pearson correlation (linear)
   - Spearman correlation (rank-based)
   - Statistical significance (p-values)
   - Survival analysis (efficacy vs. PFS/OS using Kaplan-Meier)
   - Visualization (scatter plots, survival curves)

**Phase 4D: Integration** (1 day)

1. **Combine Results**
   - Run both benchmarks (cBioPortal + Published RCT)
   - Compare results (which approach gives better correlation?)
   - Document findings

2. **Integrate with Existing Benchmarks**
   - Add to benchmark suite
   - Standardize output format
   - Add to CI/CD pipeline

#### Limitations & Caveats:

1. **Trial Population Matching**
   - **Challenge**: Published trials report aggregate results, not individual patient data
   - **Solution**: Use trial inclusion criteria to construct representative patient population
   - **Limitation**: May not perfectly match actual trial population

2. **Response Rate vs. Efficacy Score**
   - **Challenge**: Response rate (0-100%) vs. efficacy score (0-1) are different scales
   - **Solution**: Normalize or use rank-based correlation (Spearman)
   - **Limitation**: Correlation doesn't mean causation

3. **Confounding Factors**
   - **Challenge**: Trial outcomes depend on many factors (line of therapy, prior treatments, etc.)
   - **Solution**: Stratify by trial characteristics (first-line, maintenance, etc.)
   - **Limitation**: May not capture all confounding factors

4. **Small Sample Size**
   - **Challenge**: Limited number of published trials with detailed biomarker data
   - **Solution**: Start with well-characterized trials (SOLO-2, PAOLA-1, etc.)
   - **Limitation**: May not be generalizable to all trials

#### Success Criteria:

- **Response Rate Correlation**: Pearson r ‚â• 0.60, Spearman œÅ ‚â• 0.55 (moderate correlation)
- **Drug Ranking Accuracy**: Top-3 accuracy ‚â• 75% (we rank best drug in top 3)
- **Mechanism Fit Correlation**: Pearson r ‚â• 0.50 (mechanism fit correlates with outcomes)
- **Statistical Significance**: p-value < 0.05 (correlation is statistically significant)

#### Files to Create:

**cBioPortal Extraction** (NEW - Using pyBioPortal):
- `scripts/benchmark/extract_cbioportal_trial_datasets.py` - Extract datasets from cBioPortal
- `scripts/benchmark/validate_cbioportal_datasets.py` - Validate extracted data
- `scripts/benchmark/benchmark_clinical_trial_outcomes_cbioportal.py` - Benchmark using cBioPortal data
- `scripts/benchmark/utils/cbioportal_client.py` - pyBioPortal wrapper
- `scripts/benchmark/utils/dataset_combiner.py` - Combine mutations + outcomes + treatments

**Published RCT Extraction**:
- `scripts/benchmark/benchmark_clinical_trial_outcomes_rct.py` - Benchmark using published RCT results
- `scripts/data/published_rct_results.json` - Trial database
- `scripts/utils/trial_population_constructor.py` - Construct patient populations from trial criteria

**Shared Utilities**:
- `scripts/benchmark/utils/correlation_analysis.py` - Correlation computation and visualization
- `scripts/benchmark/utils/survival_analysis.py` - Survival analysis (Kaplan-Meier, Cox regression)

---

### Phase 5: Add Comparison Utilities (Low Priority)

**Goal**: Make it easy to compare benchmark runs

#### Changes Needed:

1. **Add Baseline Comparison**
   ```python
   def compare_to_baseline(current_file: str, baseline_file: str) -> Dict:
       """Compare current benchmark results to baseline."""
       # Load both files
       # Compare metrics
       # Show delta (improvement/degradation)
       # Return comparison report
   ```

2. **Add Trend Analysis**
   ```python
   def analyze_benchmark_trends(results_dir: str) -> Dict:
       """Analyze benchmark results over time."""
       # Load all benchmark files
       # Compute trends (improving/degrading)
       # Show regression alerts
       # Return trend report
   ```

3. **Add Visualization**
   - Plot metrics over time
   - Show pass/fail trends
   - Highlight regressions

---

## üìã Implementation Checklist

### Phase 1: Standardize Output Format
- [ ] Update `benchmark_mbd4_tp53_accuracy.py` output format
- [ ] Add metrics aggregation (per dimension)
- [ ] Add targets section
- [ ] Add provenance section
- [ ] Test output format matches SOTA structure
- [ ] Update `benchmark_clinical_validation.py` to match
- [ ] Update `benchmark_brca_tp53_proxy.py` to match

### Phase 2: Simplify Structure
- [ ] Extract API call functions (if not already done)
- [ ] Simplify test methods (<50 lines each)
- [ ] Add clear test organization comments
- [ ] Review and refactor complex methods

### Phase 3: Improve Console Output
- [ ] Add progress indicators
- [ ] Add summary table
- [ ] Improve error messages
- [ ] Add emoji indicators (‚úÖ/‚ùå) for clarity
- [ ] Test console output readability

### Phase 4: Clinical Trial Outcome Benchmarking (NEW)
- [ ] **Phase 4A: Extract cBioPortal Datasets** (Using pyBioPortal)
  - [ ] Create extraction script (`extract_cbioportal_trial_datasets.py`)
    - [ ] Find target studies (ovarian cancer)
    - [ ] Extract mutations using `pybioportal.mutations`
    - [ ] Extract clinical outcomes using `pybioportal.clinical_data`
    - [ ] Extract treatments using `pybioportal.treatments`
    - [ ] Combine into unified dataset
  - [ ] Create dataset validator (`validate_cbioportal_datasets.py`)
    - [ ] Check data completeness
    - [ ] Filter to patients with complete data
    - [ ] Report data quality metrics
  - [ ] Test extraction on `ov_tcga_pan_can_atlas_2018`
    - [ ] Verify data structure
    - [ ] Check data completeness
- [ ] **Phase 4B: Extract Published RCT Results**
  - [ ] Create trial database (`published_rct_results.json`)
    - [ ] Extract SOLO-2, PAOLA-1, PRIMA, ARIEL3, NOVA results
    - [ ] Map to patient populations (mutations, biomarkers)
    - [ ] Structure as JSON database
- [ ] **Phase 4C: Create Benchmark Scripts**
  - [ ] Create cBioPortal benchmark (`benchmark_clinical_trial_outcomes_cbioportal.py`)
    - [ ] Load cBioPortal datasets
    - [ ] For each patient: Run `/api/efficacy/predict`
    - [ ] Compare predictions vs. actual outcomes (individual patient level)
    - [ ] Compute correlation metrics (Pearson, Spearman)
  - [ ] Create RCT benchmark (`benchmark_clinical_trial_outcomes_rct.py`)
    - [ ] Load published RCT results
    - [ ] For each trial: Run our system on trial population
    - [ ] Compare predictions vs. published outcomes (aggregate level)
    - [ ] Compute correlation metrics
  - [ ] Add correlation analysis utilities
    - [ ] Pearson correlation (linear)
    - [ ] Spearman correlation (rank-based)
    - [ ] Statistical significance (p-values)
    - [ ] Survival analysis (Kaplan-Meier, Cox regression)
    - [ ] Visualization (scatter plots, survival curves)
- [ ] **Phase 4D: Integration**
  - [ ] Combine results (cBioPortal + Published RCT)
  - [ ] Compare approaches (which gives better correlation?)
  - [ ] Document findings
  - [ ] Integrate with existing benchmarks
    - [ ] Add to benchmark suite
    - [ ] Standardize output format
    - [ ] Add to CI/CD pipeline

### Phase 5: Add Comparison Utilities
- [ ] Create `compare_to_baseline()` function
- [ ] Create `analyze_benchmark_trends()` function
- [ ] Add visualization (optional)
- [ ] Document comparison utilities

---

## üéØ Key Differences: AYESHA vs SOTA vs Clinical Trial Outcomes

| Aspect | SOTA Benchmarks | AYESHA Benchmarks | Clinical Trial Outcomes | Recommendation |
|--------|----------------|-------------------|------------------------|----------------|
| **Structure** | Function-based | Class-based | Class-based | Keep class-based for AYESHA and Trial Outcomes |
| **Metrics** | Single metric | Multiple metrics (5 dimensions) | Correlation metrics (Pearson, Spearman) | Different metrics for different purposes |
| **Test Cases** | 2-7 variants | 1 case (MBD4+TP53) | 10-20 published trials | Trial outcomes: real-world validation |
| **Ground Truth** | Known biology | Biology assumptions | Published RCT outcomes | Trial outcomes: real patient data |
| **Output Format** | Standardized JSON | Detailed JSON | Standardized JSON + correlation | Standardize all formats |
| **Validation Type** | Performance | Consistency & alignment | Real-world accuracy | Trial outcomes: closest to real validation |
| **Error Handling** | Skip on error | Report all errors | Report all errors | Keep comprehensive reporting |

---

## üí° Key Insights

1. **SOTA benchmarks are simpler** because they test one thing (pathway alignment, AUROC, drug ranking)
2. **AYESHA benchmarks are more complex** because they test multiple dimensions (pathway, drug, mechanism, synthetic lethality, evidence)
3. **Both approaches are valid** - SOTA for performance validation, AYESHA for comprehensive consistency validation
4. **Main improvement**: Standardize output format so both can be compared/analyzed together
5. **Clinical trial outcome benchmarking** provides real-world accuracy validation (correlates predictions with published outcomes)
6. **Trial outcome benchmarking** is the bridge between consistency checks (current) and real patient outcome validation (future)

---

## üöÄ Next Steps

1. **Start with Phase 1** (Standardize Output Format) - Highest priority
2. **Then Phase 4** (Clinical Trial Outcome Benchmarking) - High value for real-world validation
3. **Then Phase 3** (Improve Console Output) - High user value
4. **Then Phase 2** (Simplify Structure) - Medium priority
5. **Finally Phase 5** (Comparison Utilities) - Nice to have

**Estimated Time**:
- Phase 1: 2-3 hours
- Phase 4: 8-10 days
  - Phase 4A (cBioPortal extraction): 2-3 days
  - Phase 4B (Published RCT extraction): 2-3 days
  - Phase 4C (Benchmark scripts): 2-3 days
  - Phase 4D (Integration): 1-2 days
- Phase 3: 1-2 hours
- Phase 2: 2-3 hours
- Phase 5: 3-4 hours
- **Total**: ~12-17 days (with Phase 4 being the major addition)

---

---

## üìä Summary: Three-Tier Benchmarking Strategy

### Tier 1: Consistency & Alignment (Current)
- **Purpose**: Validate system works as designed
- **Tests**: Pathway accuracy, drug recommendations, mechanism vectors
- **Ground Truth**: Biological assumptions + NCCN guidelines
- **Status**: ‚úÖ Ready (MBD4+TP53 case)

### Tier 2: Clinical Trial Outcomes (NEW - Phase 4)
- **Purpose**: Validate predictions correlate with real trial outcomes
- **Tests**: Response rate correlation, PFS correlation, drug ranking accuracy
- **Ground Truth**: Published RCT results (SOLO-2, PAOLA-1, PRIMA, etc.)
- **Status**: üìã Planned (6-8 days to implement)

### Tier 3: Real Patient Outcomes (Future)
- **Purpose**: Validate predictions match actual patient responses
- **Tests**: Individual patient outcome correlation
- **Ground Truth**: Real patient data (when available)
- **Status**: ‚è≥ Future (requires patient data access)

### Why Three Tiers?

1. **Tier 1 (Consistency)**: Fast, always available, validates system logic
2. **Tier 2 (Trial Outcomes)**: Real-world validation using published data, bridges gap between consistency and real outcomes
3. **Tier 3 (Patient Outcomes)**: Ultimate validation, but requires patient data access

**Current Focus**: Implement Tier 2 (Clinical Trial Outcomes) to add real-world validation layer.

---

**Last Updated**: January 27, 2025  
**Status**: üìã Planning Complete - Ready for Implementation  
**New Addition**: Phase 4 (Clinical Trial Outcome Benchmarking) - High Priority

